{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import codecs\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr0/home/glample/Research/IncrementalMT/SanDeepLearn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from recurrent import LSTM\n",
    "from layer import FullyConnectedLayer, EmbeddingLayer\n",
    "from optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_oracle = '/usr0/home/glample/Research/IncrementalMT/oracle.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [line.strip().split(' ||| ') for line in codecs.open(path_to_oracle, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "curr_sentence_pointer = 0\n",
    "for ind, line in enumerate(lines):\n",
    "    if len(line) == 1 and line[0] == '':\n",
    "        sentences.append(lines[curr_sentence_pointer:ind])\n",
    "        curr_sentence_pointer = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_words = set()\n",
    "for sentence in sentences:\n",
    "    source_sentence = sentence[0][0]\n",
    "    for word in source_sentence.split():\n",
    "        source_words.add(word)\n",
    "source_words.add('<s>')\n",
    "source_words.add('</s>')\n",
    "source_words.add('$NONE$')\n",
    "source_words.add('<ss>')\n",
    "source_words.add('</se>')\n",
    "source_words.add('<fcs>')\n",
    "source_words.add('</fcs>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_words = set()\n",
    "for sentence in sentences:\n",
    "    target_sentence = sentence[1][0]\n",
    "    for word in target_sentence.split():\n",
    "        target_words.add(word)\n",
    "target_words.add('<s>')\n",
    "target_words.add('</s>')\n",
    "target_words.add('$NONE$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    for state in sentence[2:]:\n",
    "        assert len(state) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_word2ind = {word:ind for ind, word in enumerate(source_words)}\n",
    "source_ind2word = {ind:word for ind, word in enumerate(source_words)}\n",
    "target_word2ind = {word:ind for ind, word in enumerate(target_words)}\n",
    "target_ind2word = {ind:word for ind, word in enumerate(target_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsource_word2ind['<s>'] = len(source_word2ind)\\nsource_word2ind['</s>'] = len(source_word2ind) + 1\\nsource_word2ind['$NONE$'] = len(source_word2ind) + 2\\nsource_word2ind['<ss>'] = len(source_word2ind) + 3\\nsource_word2ind['</se>'] = len(source_word2ind) + 4\\nsource_word2ind['<fcs>'] = len(source_word2ind) + 5\\nsource_word2ind['</fce>'] = len(source_word2ind) + 6\\n\\ntarget_word2ind['<s>'] = len(target_word2ind)\\ntarget_word2ind['</s>'] = len(target_word2ind) + 1\\ntarget_word2ind['$NONE$'] = len(target_word2ind) + 2\\n\\nsource_ind2word[len(source_word2ind)] = '<s>'\\nsource_ind2word[len(source_word2ind) + 1] = '</s>'\\nsource_ind2word[len(source_word2ind) + 2] = '$NONE$'\\nsource_ind2word[len(source_word2ind) + 3] = '<ss>'\\nsource_ind2word[len(source_word2ind) + 4] = '</se>'\\nsource_ind2word[len(source_word2ind) + 5] = '<fcs>'\\nsource_ind2word[len(source_word2ind) + 6] = '</fce>'\\n\\ntarget_ind2word[len(target_word2ind)] = '<s>' \\ntarget_ind2word[len(target_word2ind) + 1] = '</s>' \\ntarget_ind2word[len(target_word2ind) + 2] = '$NONE$'\\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "source_word2ind['<s>'] = len(source_word2ind)\n",
    "source_word2ind['</s>'] = len(source_word2ind) + 1\n",
    "source_word2ind['$NONE$'] = len(source_word2ind) + 2\n",
    "source_word2ind['<ss>'] = len(source_word2ind) + 3\n",
    "source_word2ind['</se>'] = len(source_word2ind) + 4\n",
    "source_word2ind['<fcs>'] = len(source_word2ind) + 5\n",
    "source_word2ind['</fce>'] = len(source_word2ind) + 6\n",
    "\n",
    "target_word2ind['<s>'] = len(target_word2ind)\n",
    "target_word2ind['</s>'] = len(target_word2ind) + 1\n",
    "target_word2ind['$NONE$'] = len(target_word2ind) + 2\n",
    "\n",
    "source_ind2word[len(source_word2ind)] = '<s>'\n",
    "source_ind2word[len(source_word2ind) + 1] = '</s>'\n",
    "source_ind2word[len(source_word2ind) + 2] = '$NONE$'\n",
    "source_ind2word[len(source_word2ind) + 3] = '<ss>'\n",
    "source_ind2word[len(source_word2ind) + 4] = '</se>'\n",
    "source_ind2word[len(source_word2ind) + 5] = '<fcs>'\n",
    "source_ind2word[len(source_word2ind) + 6] = '</fce>'\n",
    "\n",
    "target_ind2word[len(target_word2ind)] = '<s>' \n",
    "target_ind2word[len(target_word2ind) + 1] = '</s>' \n",
    "target_ind2word[len(target_word2ind) + 2] = '$NONE$'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_input = T.ivector()\n",
    "forward_context_input = T.ivector()\n",
    "target_input = T.ivector()\n",
    "action_prediction = T.scalar()\n",
    "target_output = T.ivector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data to test dimensions\n",
    "syn_stack_input = np.random.randint(low=0, high=len(source_word2ind), size=(5,)).astype(np.int32)\n",
    "#syn_stack_input = np.array([61699, 49379, 61701]).astype(np.int32)\n",
    "syn_forward_context_input = np.random.randint(low=0, high=len(source_word2ind), size=(6,)).astype(np.int32)\n",
    "syn_action_prediction = np.random.randint(low=0, high=2, size=(1,)).astype(np.float32)[0]\n",
    "syn_target_input = np.random.randint(low=0, high=len(target_word2ind), size=(7,)).astype(np.int32)\n",
    "syn_target_output = np.random.randint(low=0, high=len(target_word2ind), size=(7,)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural Inventory\n",
    "\n",
    "# Model Parameters\n",
    "src_emb_dim      = 256  # source word embedding dimension\n",
    "tgt_emb_dim      = 256  # target word embedding dimension\n",
    "src_lstm_hid_dim = 512  # source LSTMs hidden dimension\n",
    "tgt_lstm_hid_dim = 2 * src_lstm_hid_dim  # target LSTM hidden dimension\n",
    "beta = 500 # RNN regularization parameter\n",
    "\n",
    "# Embedding Layers\n",
    "source_embedding = EmbeddingLayer(input_dim=len(source_word2ind) + 1, output_dim=src_emb_dim)\n",
    "target_embedding = EmbeddingLayer(input_dim=len(target_word2ind) + 1, output_dim=tgt_emb_dim)\n",
    "\n",
    "# Stack BiLSTM\n",
    "stack_source_lstm_forward = LSTM(input_dim=src_emb_dim, output_dim=src_lstm_hid_dim, name='source_stack_lstm_forward')\n",
    "stack_source_lstm_backward = LSTM(input_dim=src_emb_dim, output_dim=src_lstm_hid_dim, name='source_stack_lstm_backward')\n",
    "\n",
    "# Forward Context BiLSTM\n",
    "forward_context_lstm_forward = LSTM(input_dim=src_emb_dim, output_dim=src_lstm_hid_dim, name='source_forward_context_lstm')\n",
    "forward_context_lstm_backward = LSTM(input_dim=src_emb_dim, output_dim=src_lstm_hid_dim, name='source_forward_context_lstm')\n",
    "\n",
    "# Target Language LSTM Decoder\n",
    "target_lstm = LSTM(input_dim = 2 * tgt_emb_dim, output_dim=tgt_lstm_hid_dim, name='target_lstm')\n",
    "\n",
    "# Action prediction Layer\n",
    "action_prediction_weights = FullyConnectedLayer(input_dim=2 * stack_source_lstm_forward.output_dim + 2 * forward_context_lstm_forward.output_dim, output_dim=1)\n",
    "\n",
    "# Target Language Word Prediction Layer\n",
    "target_word_decoding_weights = FullyConnectedLayer(input_dim=tgt_lstm_hid_dim + 2 * src_lstm_hid_dim, output_dim=len(target_word2ind), activation='softmax')\n",
    "\n",
    "# Layer to project the Encoder representation to the same size as h_0 of the decoder LSTM\n",
    "projection_weights = FullyConnectedLayer(input_dim = 2 * src_lstm_hid_dim, output_dim = tgt_emb_dim, activation='tanh')\n",
    "\n",
    "# Set model parameters\n",
    "action_params = source_embedding.params + stack_source_lstm_forward.params + stack_source_lstm_backward.params + forward_context_lstm_forward.params + forward_context_lstm_backward.params + action_prediction_weights.params\n",
    "seq_seq_params = source_embedding.params + stack_source_lstm_forward.params + stack_source_lstm_backward.params + target_embedding.params + target_lstm.params[:-1] + projection_weights.params + target_word_decoding_weights.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack_embedding_matrix [[ 0.32862642 -0.50916559  0.2742027  ..., -0.89670622 -0.53259957\n",
      "  -0.68092984]\n",
      " [ 0.97198701 -0.53723347 -0.80460501 ..., -0.81999165 -0.1520461  -0.139009  ]\n",
      " [-0.32630369 -0.38549203  0.49874866 ..., -0.73710901 -0.41236058\n",
      "  -0.32732522]\n",
      " [-0.10473968 -0.67169613  0.55053246 ...,  0.66723001 -0.58391631\n",
      "  -0.68664545]\n",
      " [-0.42100394  0.77031434 -0.56798548 ..., -0.0388076   0.36613047\n",
      "   0.29591563]]\n",
      "stack_lstm_representation (5, 1024)\n",
      "forward_context_lstm_representation (6, 1024)\n",
      "concatenated_representation (2048,)\n",
      "predicton 0.499748587608\n",
      "action_prediction_loss 0.249748647213\n",
      "target_embeddings (7, 256)\n",
      "target_lstm_h0 (1024,)\n",
      "repeated_src_context (7, 1024)\n",
      "repeated_src_context (7, 256)\n",
      "target_embeddings (7, 512)\n",
      "target_lstm (7, 1024)\n",
      "transition (7, 1024)\n",
      "decoded_words (7, 71180)\n",
      "decoding_loss 11.2579612732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Appened source representation to input at every step\\nrepeated_src_context = T.repeat(stack_lstm_representation[-1].dimshuffle('x', 0), target_embeddings.shape[0], axis=0)\\n#repeated_src_context = proj_layer2.link(repeated_src_context)\\ntarget_embeddings = T.concatenate((target_embeddings, repeated_src_context), axis=1)\\n\\n# Get the decoded sentence by connecting Encoder & Decoder\\nconnection = projection_weights.link(stack_lstm_representation[-1])\\nprint 'connection', connection.eval({stack_input:syn_stack_input}).shape\\ntarget_lstm.h_0 = connection\\ntarget_lstm.link(target_embeddings)\\nprint 'target_lstm', target_lstm.h.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\\n\\ntransition = target_lstm.h.dot(stack_lstm_representation.transpose())\\ntransition = transition.dot(stack_lstm_representation)\\nprint 'transition', transition.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\\n\\n# Decode words\\ndecoded_words = target_word_decoding_weights.link(target_lstm.h)\\nprint 'decoded_words', decoded_words.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\\n\\n# Compute seq-seq loss\\ndecoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()\\n\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# The computational graph for this method\n",
    "# =====================================================\n",
    "\n",
    "# Get the embedding matrices seq_len x embdding_dim\n",
    "stack_embedding_matrix = source_embedding.fprop(stack_input)\n",
    "print 'stack_embedding_matrix', stack_embedding_matrix.eval({stack_input:syn_stack_input}).shape\n",
    "forward_context_embedding_matrix = source_embedding.fprop(forward_context_input)\n",
    "\n",
    "# ===========================================================\n",
    "# Get LSTM representations of the stack, forward context\n",
    "# ===========================================================\n",
    "\n",
    "# Stack Representation\n",
    "stack_source_lstm_forward.fprop(stack_embedding_matrix)\n",
    "stack_source_lstm_backward.fprop(stack_embedding_matrix[::-1, :])\n",
    "stack_lstm_representation = T.concatenate((stack_source_lstm_forward.h, stack_source_lstm_backward.h[::-1,:]), axis=1)\n",
    "print 'stack_lstm_representation', stack_lstm_representation.eval({stack_input:syn_stack_input}).shape\n",
    "\n",
    "# Forward Context Representation\n",
    "forward_context_lstm_forward.fprop(forward_context_embedding_matrix)\n",
    "forward_context_lstm_backward.fprop(forward_context_embedding_matrix[::-1, :])\n",
    "forward_context_representation = T.concatenate((forward_context_lstm_forward.h, forward_context_lstm_backward.h[::-1,:]), axis=1)\n",
    "print 'forward_context_lstm_representation', forward_context_representation.eval({forward_context_input:syn_forward_context_input}).shape\n",
    "\n",
    "# Concatenate representations and make a prediction about what action to take\n",
    "concatenated_representation = T.concatenate((stack_lstm_representation[-1], forward_context_representation[-1]))\n",
    "print 'concatenated_representation', concatenated_representation.eval({stack_input:syn_stack_input, forward_context_input:syn_forward_context_input}).shape\n",
    "prediction = action_prediction_weights.fprop(concatenated_representation).mean() # .mean() is a hack to make it a scalar (since its only a single value, it shouldn't matter)\n",
    "print 'predicton', prediction.eval({stack_input:syn_stack_input, forward_context_input:syn_forward_context_input})\n",
    "\n",
    "# Compute squared-error loss between predicted action and gold action\n",
    "action_prediction_loss = ((action_prediction - prediction) ** 2).mean()\n",
    "print 'action_prediction_loss', action_prediction_loss.eval({stack_input:syn_stack_input, forward_context_input:syn_forward_context_input, action_prediction:syn_action_prediction})\n",
    "\n",
    "# Get target input embeddings\n",
    "target_embeddings = target_embedding.fprop(target_input)\n",
    "print 'target_embeddings', target_embeddings.eval({target_input:syn_target_input}).shape\n",
    "\n",
    "target_lstm.h_0 = stack_lstm_representation[-1]\n",
    "print 'target_lstm_h0', target_lstm.h_0.eval({stack_input:syn_stack_input}).shape\n",
    "repeated_src_context = T.repeat(stack_lstm_representation[-1].dimshuffle('x', 0), target_embeddings.shape[0], axis=0)\n",
    "print 'repeated_src_context', repeated_src_context.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "repeated_src_context = projection_weights.fprop(repeated_src_context)\n",
    "print 'repeated_src_context', repeated_src_context.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "target_embeddings = T.concatenate((target_embeddings, repeated_src_context), axis=1)\n",
    "print 'target_embeddings', target_embeddings.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "target_lstm.fprop(target_embeddings)\n",
    "print 'target_lstm', target_lstm.h.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "transition = target_lstm.h.dot(stack_lstm_representation.transpose())\n",
    "transition = transition.dot(stack_lstm_representation)\n",
    "print 'transition', transition.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "transition_last = T.concatenate([transition, target_lstm.h], axis=1)\n",
    "decoded_words = target_word_decoding_weights.fprop(transition_last)\n",
    "print 'decoded_words', decoded_words.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "decoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()\n",
    "decoding_loss += beta * T.mean((target_lstm.h[:-1] ** 2 - target_lstm.h[1:] ** 2) ** 2) # Regularization of RNNs from http://arxiv.org/pdf/1511.08400v6.pdf\n",
    "\n",
    "print 'decoding_loss', decoding_loss.eval({stack_input:syn_stack_input, target_input:syn_target_input, target_output:syn_target_output})\n",
    "\n",
    "'''\n",
    "# Appened source representation to input at every step\n",
    "repeated_src_context = T.repeat(stack_lstm_representation[-1].dimshuffle('x', 0), target_embeddings.shape[0], axis=0)\n",
    "#repeated_src_context = proj_layer2.link(repeated_src_context)\n",
    "target_embeddings = T.concatenate((target_embeddings, repeated_src_context), axis=1)\n",
    "\n",
    "# Get the decoded sentence by connecting Encoder & Decoder\n",
    "connection = projection_weights.link(stack_lstm_representation[-1])\n",
    "print 'connection', connection.eval({stack_input:syn_stack_input}).shape\n",
    "target_lstm.h_0 = connection\n",
    "target_lstm.link(target_embeddings)\n",
    "print 'target_lstm', target_lstm.h.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "transition = target_lstm.h.dot(stack_lstm_representation.transpose())\n",
    "transition = transition.dot(stack_lstm_representation)\n",
    "print 'transition', transition.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "# Decode words\n",
    "decoded_words = target_word_decoding_weights.link(target_lstm.h)\n",
    "print 'decoded_words', decoded_words.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "# Compute seq-seq loss\n",
    "decoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update parameters using ADAM\n",
    "'''\n",
    "updates_action = Optimizer(clip=5.0).adam(action_prediction_loss, action_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update parameters using ADAM\n",
    "'''\n",
    "updates_seq_seq = Optimizer(clip=5.0).adam(decoding_loss, seq_seq_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train_action = theano.function(\n",
    "    inputs=[stack_input, forward_context_input, action_prediction],\n",
    "    outputs=action_prediction_loss,\n",
    "    updates=updates_action\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train_seq_seq = theano.function(\n",
    "    inputs=[stack_input, target_input, target_output],\n",
    "    outputs=decoding_loss,\n",
    "    updates=updates_seq_seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_get_action = theano.function(\n",
    "    inputs=[stack_input, forward_context_input],\n",
    "    outputs=prediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_eval_seq_seq = theano.function(\n",
    "    inputs=[stack_input, target_input],\n",
    "    outputs=decoded_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 out of 155362 action loss 0.105259 \n",
      "Sentence 0 out of 155362 seq-seq loss 9.160011 \n",
      "Sentence 1 out of 155362 action loss 0.238035 "
     ]
    }
   ],
   "source": [
    "for ind, sentence in enumerate(sentences):\n",
    "    action_losses = []\n",
    "    seq_seq_losses = []\n",
    "    for state in sentence[2:]:\n",
    "        word = state[0]\n",
    "        action = state[1]\n",
    "        stack_state = state[2].strip().split()\n",
    "        forward_context = state[3].strip().split()\n",
    "        translation = state[4].strip().split()\n",
    "        stack_words = [source_word2ind['<ss>']] + [source_word2ind[word] for word in stack_state] + [source_word2ind['</se>']]\n",
    "        forward_context_words = [source_word2ind['<fcs>']] + [source_word2ind[word] for word in forward_context][::-1] + [source_word2ind['</fcs>']]\n",
    "        action = 1.0 if action == 'T' else 0.0\n",
    "        translation_words = [target_word2ind['<s>']] + [target_word2ind[word] for word in translation] + [target_word2ind['</s>']]\n",
    "        action_loss = f_train_action(stack_words, forward_context_words, action)\n",
    "        seq_seq_loss = f_train_seq_seq(stack_words, translation_words[:-1], translation_words[1:])\n",
    "        action_losses.append(action_loss)\n",
    "        seq_seq_losses.append(seq_seq_loss)\n",
    "    print 'Sentence %d out of %d action loss %f ' % (ind, len(sentences), np.mean(action_losses))\n",
    "    print 'Sentence %d out of %d seq-seq loss %f ' % (ind, len(sentences), np.mean(seq_seq_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.7847856879234314, dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_get_action(stack_words, forward_context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'European',\n",
       " u'Parliament',\n",
       " u',',\n",
       " u'but',\n",
       " u'only',\n",
       " u'29',\n",
       " u'%',\n",
       " u'trust',\n",
       " u'the',\n",
       " u'French',\n",
       " u'parliament',\n",
       " u';',\n",
       " u'while',\n",
       " u'the',\n",
       " u'corresponding',\n",
       " u'figures',\n",
       " u'for',\n",
       " u'Germany',\n",
       " u'are',\n",
       " u'51',\n",
       " u'%',\n",
       " u'and',\n",
       " u'23',\n",
       " u'%',\n",
       " u'.']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61703,\n",
       " 636,\n",
       " 33199,\n",
       " 28762,\n",
       " 26617,\n",
       " 19920,\n",
       " 42227,\n",
       " 61024,\n",
       " 30863,\n",
       " 52182,\n",
       " 10935,\n",
       " 32756,\n",
       " 46977,\n",
       " 27599,\n",
       " 23117,\n",
       " 774,\n",
       " 59432,\n",
       " 41971,\n",
       " 23990,\n",
       " 61705]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-103c9e3bffae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "print translation.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71181, 71179]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
