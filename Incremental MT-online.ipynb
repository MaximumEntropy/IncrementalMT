{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import codecs\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu1\")\n",
    "from collections import Counter\n",
    "import math\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr0/home/glample/Research/perso/UltraDeep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from network import LSTM\n",
    "from layer import HiddenLayer, EmbeddingLayer\n",
    "from learning_method import LearningMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_oracle = '/usr0/home/glample/Research/IncrementalMT/oracle.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [line.strip().split(' ||| ') for line in codecs.open(path_to_oracle, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "curr_sentence_pointer = 0\n",
    "for ind, line in enumerate(lines):\n",
    "    if len(line) == 1 and line[0] == '':\n",
    "        sentences.append(lines[curr_sentence_pointer:ind])\n",
    "        curr_sentence_pointer = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = sentences[:int(0.95 * len(sentences))]\n",
    "dev_sentences = sentences[int(0.95 * len(sentences)):int(0.97 * len(sentences))]\n",
    "test_sentences = sentences[int(0.97 * len(sentences)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_words = set()\n",
    "for sentence in sentences:\n",
    "    source_sentence = sentence[0][0]\n",
    "    for word in source_sentence.split():\n",
    "        source_words.add(word)\n",
    "source_words.add('<s>')\n",
    "source_words.add('</s>')\n",
    "source_words.add('$NONE$')\n",
    "source_words.add('<ss>')\n",
    "source_words.add('</se>')\n",
    "source_words.add('<fcs>')\n",
    "source_words.add('</fcs>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_words = set()\n",
    "for sentence in sentences:\n",
    "    target_sentence = sentence[1][0]\n",
    "    for word in target_sentence.split():\n",
    "        target_words.add(word)\n",
    "target_words.add('<s>')\n",
    "target_words.add('</s>')\n",
    "target_words.add('$NONE$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    for state in sentence[2:]:\n",
    "        assert len(state) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_word2ind = {word:ind for ind, word in enumerate(source_words)}\n",
    "source_ind2word = {ind:word for ind, word in enumerate(source_words)}\n",
    "target_word2ind = {word:ind for ind, word in enumerate(target_words)}\n",
    "target_ind2word = {ind:word for ind, word in enumerate(target_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsource_word2ind['<s>'] = len(source_word2ind)\\nsource_word2ind['</s>'] = len(source_word2ind) + 1\\nsource_word2ind['$NONE$'] = len(source_word2ind) + 2\\nsource_word2ind['<ss>'] = len(source_word2ind) + 3\\nsource_word2ind['</se>'] = len(source_word2ind) + 4\\nsource_word2ind['<fcs>'] = len(source_word2ind) + 5\\nsource_word2ind['</fce>'] = len(source_word2ind) + 6\\n\\ntarget_word2ind['<s>'] = len(target_word2ind)\\ntarget_word2ind['</s>'] = len(target_word2ind) + 1\\ntarget_word2ind['$NONE$'] = len(target_word2ind) + 2\\n\\nsource_ind2word[len(source_word2ind)] = '<s>'\\nsource_ind2word[len(source_word2ind) + 1] = '</s>'\\nsource_ind2word[len(source_word2ind) + 2] = '$NONE$'\\nsource_ind2word[len(source_word2ind) + 3] = '<ss>'\\nsource_ind2word[len(source_word2ind) + 4] = '</se>'\\nsource_ind2word[len(source_word2ind) + 5] = '<fcs>'\\nsource_ind2word[len(source_word2ind) + 6] = '</fce>'\\n\\ntarget_ind2word[len(target_word2ind)] = '<s>' \\ntarget_ind2word[len(target_word2ind) + 1] = '</s>' \\ntarget_ind2word[len(target_word2ind) + 2] = '$NONE$'\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "source_word2ind['<s>'] = len(source_word2ind)\n",
    "source_word2ind['</s>'] = len(source_word2ind) + 1\n",
    "source_word2ind['$NONE$'] = len(source_word2ind) + 2\n",
    "source_word2ind['<ss>'] = len(source_word2ind) + 3\n",
    "source_word2ind['</se>'] = len(source_word2ind) + 4\n",
    "source_word2ind['<fcs>'] = len(source_word2ind) + 5\n",
    "source_word2ind['</fce>'] = len(source_word2ind) + 6\n",
    "\n",
    "target_word2ind['<s>'] = len(target_word2ind)\n",
    "target_word2ind['</s>'] = len(target_word2ind) + 1\n",
    "target_word2ind['$NONE$'] = len(target_word2ind) + 2\n",
    "\n",
    "source_ind2word[len(source_word2ind)] = '<s>'\n",
    "source_ind2word[len(source_word2ind) + 1] = '</s>'\n",
    "source_ind2word[len(source_word2ind) + 2] = '$NONE$'\n",
    "source_ind2word[len(source_word2ind) + 3] = '<ss>'\n",
    "source_ind2word[len(source_word2ind) + 4] = '</se>'\n",
    "source_ind2word[len(source_word2ind) + 5] = '<fcs>'\n",
    "source_ind2word[len(source_word2ind) + 6] = '</fce>'\n",
    "\n",
    "target_ind2word[len(target_word2ind)] = '<s>' \n",
    "target_ind2word[len(target_word2ind) + 1] = '</s>' \n",
    "target_ind2word[len(target_word2ind) + 2] = '$NONE$'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_input = T.ivector()\n",
    "forward_context_input = T.ivector()\n",
    "target_input = T.ivector()\n",
    "gold_action = T.scalar()\n",
    "target_output = T.ivector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data to test dimensions\n",
    "syn_stack_input = np.random.randint(low=0, high=len(source_word2ind), size=(5,)).astype(np.int32)\n",
    "#syn_stack_input = np.array([61699, 49379, 61701]).astype(np.int32)\n",
    "syn_forward_context_input = np.random.randint(low=0, high=len(source_word2ind), size=(6,)).astype(np.int32)\n",
    "syn_action_prediction = np.random.randint(low=0, high=2, size=(1,)).astype(np.float32)[0]\n",
    "syn_target_input = np.random.randint(low=0, high=len(target_word2ind), size=(7,)).astype(np.int32)\n",
    "syn_target_output = np.random.randint(low=0, high=len(target_word2ind), size=(7,)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural Inventory\n",
    "\n",
    "# Model Parameters\n",
    "src_emb_dim      = 256  # source word embedding dimension\n",
    "tgt_emb_dim      = 256  # target word embedding dimension\n",
    "src_lstm_hid_dim = 512  # source LSTMs hidden dimension\n",
    "tgt_lstm_hid_dim = 2 * 512  # target LSTM hidden dimension\n",
    "beta = 500 # RNN regularization parameter\n",
    "\n",
    "# Embedding Layers\n",
    "source_embedding = EmbeddingLayer(input_dim=len(source_word2ind) + 1, output_dim=src_emb_dim)\n",
    "target_embedding = EmbeddingLayer(input_dim=len(target_word2ind) + 1, output_dim=tgt_emb_dim)\n",
    "\n",
    "# Stack BiLSTM\n",
    "stack_source_lstm_forward = LSTM(input_dim=src_emb_dim, hidden_dim=src_lstm_hid_dim, name='source_stack_lstm_forward', with_batch=False)\n",
    "stack_source_lstm_backward = LSTM(input_dim=src_emb_dim, hidden_dim=src_lstm_hid_dim, name='source_stack_lstm_backward', with_batch=False)\n",
    "\n",
    "# Forward Context BiLSTM\n",
    "forward_context_lstm = LSTM(input_dim=src_emb_dim, hidden_dim=src_lstm_hid_dim, name='source_forward_context_lstm', with_batch=False)\n",
    "\n",
    "# Target Language LSTM Decoder\n",
    "target_lstm = LSTM(input_dim = tgt_emb_dim, hidden_dim=tgt_lstm_hid_dim, name='target_lstm', with_batch=False)\n",
    "\n",
    "# Action prediction Layer\n",
    "action_prediction_weights = HiddenLayer(input_dim=2 * stack_source_lstm_forward.hidden_dim + forward_context_lstm.hidden_dim, output_dim=1)\n",
    "\n",
    "# Target Language Word Prediction Layer\n",
    "target_word_decoding_weights = HiddenLayer(input_dim=tgt_lstm_hid_dim + 2 * src_lstm_hid_dim, output_dim=len(target_word2ind), activation='softmax')\n",
    "\n",
    "# Layer to project the Encoder representation to the same size as h_0 of the decoder LSTM\n",
    "projection_weights = HiddenLayer(input_dim = 2 * src_lstm_hid_dim, output_dim = tgt_lstm_hid_dim, activation='tanh')\n",
    "\n",
    "# Set model parameters\n",
    "action_params = source_embedding.params + stack_source_lstm_forward.params + stack_source_lstm_backward.params + forward_context_lstm.params + action_prediction_weights.params\n",
    "seq_seq_params = source_embedding.params + stack_source_lstm_forward.params + stack_source_lstm_backward.params + target_embedding.params + target_lstm.params[:-1] + projection_weights.params + target_word_decoding_weights.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack_embedding_matrix (5, 256)\n",
      "stack_lstm_representation (5, 1024)\n",
      "forward_context_lstm_representation (6, 512)\n",
      "concatenated_representation (1536,)\n",
      "prediction 0.499724894762\n",
      "action_prediction_loss 0.249724969268\n",
      "target_embeddings (7, 256)\n",
      "target_lstm_h0 (1024,)\n",
      "target_lstm (7, 1024)\n",
      "transition (7, 1024)\n",
      "decoded_words (7, 71180)\n",
      "decoding_loss 11.1729450226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Appened source representation to input at every step\\nrepeated_src_context = T.repeat(stack_lstm_representation[-1].dimshuffle('x', 0), target_embeddings.shape[0], axis=0)\\n#repeated_src_context = proj_layer2.link(repeated_src_context)\\ntarget_embeddings = T.concatenate((target_embeddings, repeated_src_context), axis=1)\\n\\n# Get the decoded sentence by connecting Encoder & Decoder\\nconnection = projection_weights.link(stack_lstm_representation[-1])\\nprint 'connection', connection.eval({stack_input:syn_stack_input}).shape\\ntarget_lstm.h_0 = connection\\ntarget_lstm.link(target_embeddings)\\nprint 'target_lstm', target_lstm.h.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\\n\\ntransition = target_lstm.h.dot(stack_lstm_representation.transpose())\\ntransition = transition.dot(stack_lstm_representation)\\nprint 'transition', transition.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\\n\\n# Decode words\\ndecoded_words = target_word_decoding_weights.link(target_lstm.h)\\nprint 'decoded_words', decoded_words.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\\n\\n# Compute seq-seq loss\\ndecoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# The computational graph for this method\n",
    "# =====================================================\n",
    "\n",
    "# Get the embedding matrices seq_len x embdding_dim\n",
    "stack_embedding_matrix = source_embedding.link(stack_input)\n",
    "print 'stack_embedding_matrix', stack_embedding_matrix.eval({stack_input:syn_stack_input}).shape\n",
    "forward_context_embedding_matrix = source_embedding.link(forward_context_input)\n",
    "\n",
    "# ===========================================================\n",
    "# Get LSTM representations of the stack, forward context\n",
    "# ===========================================================\n",
    "\n",
    "# Stack Representation\n",
    "stack_source_lstm_forward.link(stack_embedding_matrix)\n",
    "stack_source_lstm_backward.link(stack_embedding_matrix[::-1, :])\n",
    "stack_lstm_representation = T.concatenate((stack_source_lstm_forward.h, stack_source_lstm_backward.h[::-1,:]), axis=1)\n",
    "print 'stack_lstm_representation', stack_lstm_representation.eval({stack_input:syn_stack_input}).shape\n",
    "\n",
    "# Forward Context Representation\n",
    "forward_context_lstm.link(forward_context_embedding_matrix[::-1, :])\n",
    "#forward_context_representation = T.concatenate((forward_context_lstm_forward.h, forward_context_lstm_backward.h[::-1,:]), axis=1)\n",
    "print 'forward_context_lstm_representation', forward_context_lstm.h.eval({forward_context_input:syn_forward_context_input}).shape\n",
    "\n",
    "# Concatenate representations and make a prediction about what action to take\n",
    "concatenated_representation = T.concatenate((stack_lstm_representation[-1], forward_context_lstm.h[-1]))\n",
    "print 'concatenated_representation', concatenated_representation.eval({stack_input:syn_stack_input, forward_context_input:syn_forward_context_input}).shape\n",
    "action_prediction = action_prediction_weights.link(concatenated_representation).mean() # .mean() is a hack to make it a scalar (since its only a single value, it shouldn't matter)\n",
    "print 'prediction', action_prediction.eval({stack_input:syn_stack_input, forward_context_input:syn_forward_context_input})\n",
    "\n",
    "# Compute squared-error loss between predicted action and gold action\n",
    "action_prediction_loss = ((gold_action - action_prediction) ** 2).mean()\n",
    "print 'action_prediction_loss', action_prediction_loss.eval({stack_input:syn_stack_input, forward_context_input:syn_forward_context_input, gold_action:syn_action_prediction})\n",
    "\n",
    "# Get target input embeddings\n",
    "target_embeddings = target_embedding.link(target_input)\n",
    "print 'target_embeddings', target_embeddings.eval({target_input:syn_target_input}).shape\n",
    "\n",
    "target_lstm.h_0 = projection_weights.link(stack_lstm_representation[-1])\n",
    "print 'target_lstm_h0', target_lstm.h_0.eval({stack_input:syn_stack_input}).shape\n",
    "#repeated_src_context = T.repeat(stack_lstm_representation[-1].dimshuffle('x', 0), target_embeddings.shape[0], axis=0)\n",
    "#print 'repeated_src_context', repeated_src_context.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "#repeated_src_context = projection_weights.fprop(repeated_src_context)\n",
    "#print 'repeated_src_context', repeated_src_context.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "#target_embeddings = T.concatenate((target_embeddings, repeated_src_context), axis=1)\n",
    "#print 'target_embeddings', target_embeddings.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "target_lstm.link(target_embeddings)\n",
    "print 'target_lstm', target_lstm.h.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "transition = target_lstm.h.dot(stack_lstm_representation.transpose())\n",
    "transition = transition.dot(stack_lstm_representation)\n",
    "print 'transition', transition.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "transition_last = T.concatenate([transition, target_lstm.h], axis=1)\n",
    "decoded_words = target_word_decoding_weights.link(transition_last)\n",
    "print 'decoded_words', decoded_words.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "decoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()\n",
    "decoding_loss += beta * T.mean((target_lstm.h[:-1] ** 2 - target_lstm.h[1:] ** 2) ** 2) # Regularization of RNNs from http://arxiv.org/pdf/1511.08400v6.pdf\n",
    "\n",
    "print 'decoding_loss', decoding_loss.eval({stack_input:syn_stack_input, target_input:syn_target_input, target_output:syn_target_output})\n",
    "\n",
    "'''\n",
    "# Appened source representation to input at every step\n",
    "repeated_src_context = T.repeat(stack_lstm_representation[-1].dimshuffle('x', 0), target_embeddings.shape[0], axis=0)\n",
    "#repeated_src_context = proj_layer2.link(repeated_src_context)\n",
    "target_embeddings = T.concatenate((target_embeddings, repeated_src_context), axis=1)\n",
    "\n",
    "# Get the decoded sentence by connecting Encoder & Decoder\n",
    "connection = projection_weights.link(stack_lstm_representation[-1])\n",
    "print 'connection', connection.eval({stack_input:syn_stack_input}).shape\n",
    "target_lstm.h_0 = connection\n",
    "target_lstm.link(target_embeddings)\n",
    "print 'target_lstm', target_lstm.h.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "transition = target_lstm.h.dot(stack_lstm_representation.transpose())\n",
    "transition = transition.dot(stack_lstm_representation)\n",
    "print 'transition', transition.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "# Decode words\n",
    "decoded_words = target_word_decoding_weights.link(target_lstm.h)\n",
    "print 'decoded_words', decoded_words.eval({stack_input:syn_stack_input, target_input:syn_target_input}).shape\n",
    "\n",
    "# Compute seq-seq loss\n",
    "decoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update parameters using ADAM\n",
    "'''\n",
    "updates_action =LearningMethod(clip=5.0).get_updates('adam', action_prediction_loss, action_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update parameters using ADAM\n",
    "'''\n",
    "updates_seq_seq = LearningMethod(clip=5.0).get_updates('adam', decoding_loss, seq_seq_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train_action = theano.function(\n",
    "    inputs=[stack_input, forward_context_input, gold_action],\n",
    "    outputs=[action_prediction_loss, action_prediction],\n",
    "    updates=updates_action\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train_seq_seq = theano.function(\n",
    "    inputs=[stack_input, target_input, target_output],\n",
    "    outputs=decoding_loss,\n",
    "    updates=updates_seq_seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_get_action = theano.function(\n",
    "    inputs=[stack_input, forward_context_input],\n",
    "    outputs=action_prediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_eval_seq_seq = theano.function(\n",
    "    inputs=[stack_input, target_input],\n",
    "    outputs=decoded_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in xrange(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in xrange(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in xrange(len(reference)+1-n)])\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis)+1-n, 0]))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(stats):\n",
    "    if len(filter(lambda x: x==0, stats)) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_bleu(hypotheses, reference):\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    #ref_lines = [line.strip().split() for line in open(path_to_dev_tgt, 'r')]\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return \"%.2f\" % (100*bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_predictions():\n",
    "    train_predictions = []    \n",
    "    for ind, sent in enumerate(train_sentences[800:815]):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_sentences)\n",
    "        sent = sent[0][0].split()\n",
    "        src_words = np.array([source_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        stack = [source_word2ind['<ss>'], source_word2ind['</se>']]\n",
    "        \n",
    "        sentence_output = []\n",
    "        for ind, word in enumerate(src_words):\n",
    "            stack.insert(-1, word) \n",
    "            forward_context = src_words[ind:]\n",
    "            forward_context_words = [source_word2ind['<fcs>']] + [x for x in forward_context] + [source_word2ind['</fcs>']]\n",
    "            current_outputs = [target_word2ind['<s>']]\n",
    "            action = f_get_action(np.array(stack).astype(np.int32), np.array(forward_context_words).astype(np.int32))\n",
    "            \n",
    "            if action > 0.5 or ind == len(src_words) -1: # If translate or last word of sentence\n",
    "                while True:\n",
    "                    next_word = f_eval_seq_seq(stack, current_outputs).argmax(axis=1)[-1]\n",
    "                    current_outputs.append(next_word)\n",
    "                    #print [target_ind2word[x] for x in current_outputs]\n",
    "                    if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                        if next_word == target_word2ind['</s>']:\n",
    "                            sentence_output.extend([target_ind2word[x] for x in current_outputs[1:-1]])\n",
    "                        else:\n",
    "                            sentence_output.extend([target_ind2word[x] for x in current_outputs[1:]])\n",
    "                        break\n",
    "                stack = [source_word2ind['<ss>'], source_word2ind['</se>']]\n",
    "            else:\n",
    "                pass\n",
    "            print u'{: <2} | {: <20} | {: <2} | {: <30} | {: <30} '.format(str(ind), source_ind2word[word], 'T' if action > 0.5 else 'S', ' '.join([source_ind2word[x] for x in stack_copy]), ' '.join([target_ind2word[x] for x in current_outputs]))\n",
    "        \n",
    "        train_predictions.append(sentence_output)\n",
    "        #print ' '.join(sentence_output)\n",
    "        print '===================================================================================='\n",
    "    return train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = ['<s>', 'the', '</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx.insert(-1, 'boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx.insert(-1, 'runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'the', 'boy', 'runs', '</s>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions():\n",
    "    validation_predictions = []\n",
    "    NUM_MAX_SHIFT = 3\n",
    "    correct_actions = 0\n",
    "    incorrect_actions = 0\n",
    "    correct_actions_t = 0\n",
    "    incorrect_actions_t = 0\n",
    "    for ind, sentence in enumerate(dev_sentences[800:815]):\n",
    "        actionset = []\n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_sentences)\n",
    "        sent = sentence[0][0].split()\n",
    "        src_words = np.array([source_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        stack = [source_word2ind['<ss>'], source_word2ind['</se>']]\n",
    "        assert len(src_words) == len(sentence[2:])\n",
    "        sentence_output = []\n",
    "        for ind, (word, gold) in enumerate(zip(src_words, sentence[2:])):\n",
    "            stack.insert(-1, word) \n",
    "            forward_context = src_words[ind:]\n",
    "            forward_context_words = [source_word2ind['<fcs>']] + [x for x in forward_context] + [source_word2ind['</fcs>']]\n",
    "            current_outputs = [target_word2ind['<s>']]\n",
    "            action = f_get_action(np.array(stack).astype(np.int32), np.array(forward_context_words).astype(np.int32))\n",
    "            actionset.append('T' if action > 0.5 else 'S')\n",
    "            if (gold[1] == 'T' and action > 0.5) or (gold[1] == 'S' and action < 0.5):\n",
    "                correct_actions += 1\n",
    "            else:\n",
    "                incorrect_actions += 1\n",
    "            if gold[1] == 'T':\n",
    "                correct_actions_t += 1\n",
    "            else:\n",
    "                incorrect_actions_t += 1\n",
    "            if action > 0.5 or ind == len(src_words) -1 or actionset[-NUM_MAX_SHIFT:] == ['S'] * NUM_MAX_SHIFT: # If translate or last word of sentence\n",
    "                while True:\n",
    "                    next_word = f_eval_seq_seq(stack, current_outputs).argmax(axis=1)[-1]\n",
    "                    current_outputs.append(next_word)\n",
    "                    #print [target_ind2word[x] for x in current_outputs]\n",
    "                    if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                        if next_word == target_word2ind['</s>']:\n",
    "                            sentence_output.extend([target_ind2word[x] for x in current_outputs[1:-1]])\n",
    "                        else:\n",
    "                            sentence_output.extend([target_ind2word[x] for x in current_outputs[1:]])\n",
    "                        break\n",
    "                stack_copy = copy.deepcopy(stack)\n",
    "                stack = [source_word2ind['<ss>'], source_word2ind['</se>']]\n",
    "            else:\n",
    "                stack_copy = copy.deepcopy(stack)\n",
    "            print u'{: <2} | {: <20} | {: <2} | {: <30} | {: <30} '.format(str(ind), source_ind2word[word], 'T' if action > 0.5 else 'S', ' '.join([source_ind2word[x] for x in stack_copy]), ' '.join([target_ind2word[x] for x in current_outputs]))\n",
    "        \n",
    "        validation_predictions.append(sentence_output)\n",
    "        #print ' '.join(sentence_output)\n",
    "        print '===================================================================================='\n",
    "    model_acc = float(correct_actions) / float(correct_actions + incorrect_actions)\n",
    "    baseline_acc = float(correct_actions_t) / float(correct_actions_t + incorrect_actions_t)\n",
    "    print 'Action prediction accuracy : %f%% ' % (model_acc)\n",
    "    print 'Action prediction accuracy Translate only : %f%% ' % (baseline_acc)\n",
    "    return validation_predictions, model_acc, baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions_noprint():\n",
    "    validation_predictions = []\n",
    "    NUM_MAX_SHIFT = 3\n",
    "    correct_actions = 0\n",
    "    incorrect_actions = 0\n",
    "    correct_actions_t = 0\n",
    "    incorrect_actions_t = 0\n",
    "    all_actions = []\n",
    "    for ind, sentence in enumerate(dev_sentences):\n",
    "        actionset = []\n",
    "        sentence_actions = []\n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_sentences)\n",
    "        sent = sentence[0][0].split()\n",
    "        src_words = np.array([source_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        stack = [source_word2ind['<ss>'], source_word2ind['</se>']]\n",
    "        assert len(src_words) == len(sentence[2:])\n",
    "        sentence_output = []\n",
    "        for ind, (word, gold) in enumerate(zip(src_words, sentence[2:])):\n",
    "            stack.insert(-1, word) \n",
    "            forward_context = src_words[ind:]\n",
    "            forward_context_words = [source_word2ind['<fcs>']] + [x for x in forward_context] + [source_word2ind['</fcs>']]\n",
    "            current_outputs = [target_word2ind['<s>']]\n",
    "            action = f_get_action(np.array(stack).astype(np.int32), np.array(forward_context_words).astype(np.int32))\n",
    "            actionset.append('T' if action > 0.5 else 'S')\n",
    "            if (gold[1] == 'T' and action > 0.5) or (gold[1] == 'S' and action < 0.5):\n",
    "                correct_actions += 1\n",
    "            else:\n",
    "                incorrect_actions += 1\n",
    "            if gold[1] == 'T':\n",
    "                correct_actions_t += 1\n",
    "            else:\n",
    "                incorrect_actions_t += 1\n",
    "            if action > 0.5 or ind == len(src_words) -1 or actionset[-NUM_MAX_SHIFT:] == ['S'] * NUM_MAX_SHIFT: # If translate or last word of sentence\n",
    "                while True:\n",
    "                    next_word = f_eval_seq_seq(stack, current_outputs).argmax(axis=1)[-1]\n",
    "                    current_outputs.append(next_word)\n",
    "                    # print [target_ind2word[x] for x in current_outputs]\n",
    "                    if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                        if next_word == target_word2ind['</s>']:\n",
    "                            sentence_output.extend([target_ind2word[x] for x in current_outputs[1:-1]])\n",
    "                        else:\n",
    "                            sentence_output.extend([target_ind2word[x] for x in current_outputs[1:]])\n",
    "                        break\n",
    "                stack_copy = copy.deepcopy(stack)\n",
    "                stack = [source_word2ind['<ss>'], source_word2ind['</se>']]\n",
    "            else:\n",
    "                stack_copy = copy.deepcopy(stack)\n",
    "            # print u'{: <2} | {: <20} | {: <2} | {: <30} | {: <30} '.format(str(ind), source_ind2word[word], 'T' if action > 0.5 else 'S', ' '.join([source_ind2word[x] for x in stack_copy]), ' '.join([target_ind2word[x] for x in current_outputs]))\n",
    "            sentence_actions.append([str(ind), source_ind2word[word], 'T' if action > 0.5 else 'S', ' '.join([source_ind2word[x] for x in stack_copy]), ' '.join([target_ind2word[x] for x in current_outputs])])\n",
    "        validation_predictions.append(sentence_output)\n",
    "        all_actions.append(sentence_actions)\n",
    "        # print ' '.join(sentence_output)\n",
    "        # print '===================================================================================='\n",
    "    model_acc = float(correct_actions) / float(correct_actions + incorrect_actions)\n",
    "    baseline_acc = float(correct_actions_t) / float(correct_actions_t + incorrect_actions_t)\n",
    "    # print 'Action prediction accuracy : %f%% ' % (model_acc)\n",
    "    # print 'Action prediction accuracy Translate only : %f%% ' % (baseline_acc)\n",
    "    return validation_predictions, model_acc, baseline_acc, all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ind, sentence in enumerate(train_sentences[:1000]):\n",
    "    trans = ''\n",
    "    for state in sentence[2:]:\n",
    "        translation = state[4].split()\n",
    "        action = state[1]\n",
    "        if action == 'T'and '$NONE$' in translation and translation != ['$NONE$']:\n",
    "            print state[2], translation\n",
    "        #if action == 'T' and translation == ['$NONE$']:\n",
    "        #    print state[2], translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_valid_bleu = -sys.maxint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_all_actions = []\n",
    "best_action_acc = -sys.maxint\n",
    "best_action_baseline = -sys.maxint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 out of 155362 action loss 0.333333 \n",
      "Sentence 0 out of 155362 seq-seq loss 3.515689 \n",
      "0 3108\n",
      "Validation BLEU : 3.73 \n",
      "Sentence 30 out of 155362 action loss 0.369681 \n",
      "Sentence 30 out of 155362 seq-seq loss 3.570181 \n",
      "Sentence 60 out of 155362 action loss 0.415441 \n",
      "Sentence 60 out of 155362 seq-seq loss 3.598959 \n",
      "Sentence 90 out of 155362 action loss 0.425649 \n",
      "Sentence 90 out of 155362 seq-seq loss 3.573635 \n",
      "Sentence 120 out of 155362 action loss 0.412776 \n",
      "Sentence 120 out of 155362 seq-seq loss 3.588873 \n",
      "Sentence 150 out of 155362 action loss 0.401475 \n",
      "Sentence 150 out of 155362 seq-seq loss 3.565289 \n",
      "Sentence 180 out of 155362 action loss 0.393246 \n",
      "Sentence 180 out of 155362 seq-seq loss 3.577946 \n",
      "Sentence 210 out of 155362 action loss 0.401812 \n",
      "Sentence 210 out of 155362 seq-seq loss 3.610161 \n",
      "Sentence 240 out of 155362 action loss 0.406521 \n",
      "Sentence 240 out of 155362 seq-seq loss 3.621521 \n",
      "Sentence 270 out of 155362 action loss 0.402627 \n",
      "Sentence 270 out of 155362 seq-seq loss 3.649221 \n",
      "Sentence 300 out of 155362 action loss 0.401261 \n",
      "Sentence 300 out of 155362 seq-seq loss 3.667156 \n",
      "Sentence 330 out of 155362 action loss 0.399832 \n",
      "Sentence 330 out of 155362 seq-seq loss 3.673498 \n",
      "Sentence 360 out of 155362 action loss 0.401883 \n",
      "Sentence 360 out of 155362 seq-seq loss 3.688183 \n",
      "Sentence 390 out of 155362 action loss 0.404454 \n",
      "Sentence 390 out of 155362 seq-seq loss 3.720718 \n",
      "Sentence 420 out of 155362 action loss 0.404780 \n",
      "Sentence 420 out of 155362 seq-seq loss 3.725817 \n",
      "Sentence 450 out of 155362 action loss 0.408134 \n",
      "Sentence 450 out of 155362 seq-seq loss 3.744505 \n",
      "Sentence 480 out of 155362 action loss 0.408558 \n",
      "Sentence 480 out of 155362 seq-seq loss 3.761237 \n",
      "0 3108\n",
      "Validation BLEU : 4.64 \n",
      "Sentence 510 out of 155362 action loss 0.409705 \n",
      "Sentence 510 out of 155362 seq-seq loss 3.738940 \n",
      "Sentence 540 out of 155362 action loss 0.408842 \n",
      "Sentence 540 out of 155362 seq-seq loss 3.758467 \n",
      "Sentence 570 out of 155362 action loss 0.409144 \n",
      "Sentence 570 out of 155362 seq-seq loss 3.777141 \n",
      "Sentence 600 out of 155362 action loss 0.408767 \n",
      "Sentence 600 out of 155362 seq-seq loss 3.778837 \n",
      "Sentence 630 out of 155362 action loss 0.407654 \n",
      "Sentence 630 out of 155362 seq-seq loss 3.772105 \n",
      "Sentence 660 out of 155362 action loss 0.407165 \n",
      "Sentence 660 out of 155362 seq-seq loss 3.774709 \n",
      "Sentence 690 out of 155362 action loss 0.408354 \n",
      "Sentence 690 out of 155362 seq-seq loss 3.787354 \n",
      "Sentence 720 out of 155362 action loss 0.409443 \n",
      "Sentence 720 out of 155362 seq-seq loss 3.793303 \n",
      "Sentence 750 out of 155362 action loss 0.410365 \n",
      "Sentence 750 out of 155362 seq-seq loss 3.786592 \n",
      "Sentence 780 out of 155362 action loss 0.409303 \n",
      "Sentence 780 out of 155362 seq-seq loss 3.774960 \n",
      "Sentence 810 out of 155362 action loss 0.408763 \n",
      "Sentence 810 out of 155362 seq-seq loss 3.765636 \n",
      "Sentence 840 out of 155362 action loss 0.407461 \n",
      "Sentence 840 out of 155362 seq-seq loss 3.765183 \n",
      "Sentence 870 out of 155362 action loss 0.406963 \n",
      "Sentence 870 out of 155362 seq-seq loss 3.767004 \n",
      "Sentence 900 out of 155362 action loss 0.408034 \n",
      "Sentence 900 out of 155362 seq-seq loss 3.768904 \n",
      "Sentence 930 out of 155362 action loss 0.407421 \n",
      "Sentence 930 out of 155362 seq-seq loss 3.758740 \n",
      "Sentence 960 out of 155362 action loss 0.407649 \n",
      "Sentence 960 out of 155362 seq-seq loss 3.755669 \n",
      "Sentence 990 out of 155362 action loss 0.405204 \n",
      "Sentence 990 out of 155362 seq-seq loss 3.751695 \n",
      "0 3108\n",
      "Validation BLEU : 3.55 \n",
      "Sentence 1020 out of 155362 action loss 0.404763 \n",
      "Sentence 1020 out of 155362 seq-seq loss 3.751250 \n",
      "Sentence 1050 out of 155362 action loss 0.406272 \n",
      "Sentence 1050 out of 155362 seq-seq loss 3.747213 \n",
      "Sentence 1080 out of 155362 action loss 0.407023 \n",
      "Sentence 1080 out of 155362 seq-seq loss 3.746084 \n",
      "Sentence 1110 out of 155362 action loss 0.407581 \n",
      "Sentence 1110 out of 155362 seq-seq loss 3.766708 \n",
      "Sentence 1140 out of 155362 action loss 0.406390 \n",
      "Sentence 1140 out of 155362 seq-seq loss 3.762621 \n",
      "Sentence 1170 out of 155362 action loss 0.405202 \n",
      "Sentence 1170 out of 155362 seq-seq loss 3.758235 \n",
      "Sentence 1200 out of 155362 action loss 0.404248 \n",
      "Sentence 1200 out of 155362 seq-seq loss 3.749019 \n",
      "Sentence 1230 out of 155362 action loss 0.403571 \n",
      "Sentence 1230 out of 155362 seq-seq loss 3.747993 \n",
      "Sentence 1260 out of 155362 action loss 0.402681 \n",
      "Sentence 1260 out of 155362 seq-seq loss 3.749451 \n",
      "Sentence 1290 out of 155362 action loss 0.405624 \n",
      "Sentence 1290 out of 155362 seq-seq loss 3.747111 \n",
      "Sentence 1320 out of 155362 action loss 0.404815 \n",
      "Sentence 1320 out of 155362 seq-seq loss 3.747267 \n",
      "Sentence 1350 out of 155362 action loss 0.405216 \n",
      "Sentence 1350 out of 155362 seq-seq loss 3.749105 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-59cc4e9e5e7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mforward_context_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msource_word2ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<fcs>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msource_word2ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforward_context\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msource_word2ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'</fcs>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'T'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0maction_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_train_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_context_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0maction_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Train seq-seq only if the model predicts 'T'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr0/home/glample/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr0/home/glample/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "action_losses = []\n",
    "seq_seq_losses = []\n",
    "train_sentences = np.random.permutation(train_sentences)\n",
    "for ind, sentence in enumerate(train_sentences):\n",
    "    for state in sentence[2:]:\n",
    "        word = state[0]\n",
    "        action = state[1]\n",
    "        stack_state = state[2].strip().split()\n",
    "        forward_context = state[3].strip().split()\n",
    "        translation = state[4].strip().split()\n",
    "        stack_words = [source_word2ind['<ss>']] + [source_word2ind[word] for word in stack_state] + [source_word2ind['</se>']]\n",
    "        forward_context_words = [source_word2ind['<fcs>']] + [source_word2ind[word] for word in forward_context] + [source_word2ind['</fcs>']]\n",
    "        action = 1.0 if action == 'T' else 0.0\n",
    "        action_loss, prediction = f_train_action(stack_words, forward_context_words, action)\n",
    "        action_losses.append(action_loss)\n",
    "        # Train seq-seq only if the model predicts 'T'\n",
    "        if action == 1.0 and translation != ['$NONE$']:\n",
    "            translation_words = [target_word2ind['<s>']] + [target_word2ind[word] for word in translation] + [target_word2ind['</s>']]\n",
    "            seq_seq_loss = f_train_seq_seq(stack_words, translation_words[:-1], translation_words[1:])\n",
    "            seq_seq_losses.append(seq_seq_loss)\n",
    "        elif action == 0.0:\n",
    "            pass\n",
    "    if ind % 30 == 0:\n",
    "        print 'Sentence %d out of %d action loss %f ' % (ind, len(sentences), np.mean(action_losses))\n",
    "        print 'Sentence %d out of %d seq-seq loss %f ' % (ind, len(sentences), np.mean(seq_seq_losses))\n",
    "    if ind % 500 == 0:\n",
    "        valid_preds, action_acc, baseline_acc, all_actions = get_validation_predictions_noprint()\n",
    "        valid_bleu = get_validation_bleu(valid_preds, [x[1][0].split() for x in dev_sentences[:300]])\n",
    "        print 'Validation BLEU : %s ' % (valid_bleu)\n",
    "        if valid_bleu > best_valid_bleu:\n",
    "            best_valid_bleu = valid_bleu\n",
    "            best_all_actions = all_actions\n",
    "            best_action_acc = action_acc\n",
    "            best_action_baseline = baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.44'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_valid_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3108\n",
      "300 3108\n",
      "600 3108\n",
      "900"
     ]
    }
   ],
   "source": [
    "valid_preds, action_acc, baseline_acc, all_actions = get_validation_predictions_noprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3108\n"
     ]
    }
   ],
   "source": [
    "valid_preds = get_validation_predictions_noprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_bleu(valid_preds, [x[1][0].split() for x in dev_sentences[800:815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3108\n",
      "0  | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "1  | be                   | T  | <ss> be </se>                  | <s> tre </s>                  \n",
      "2  | sure                 | T  | <ss> sure </se>                | <s> comment </s>               \n",
      "3  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "4  | Russia               | T  | <ss> Russia </se>              | <s> la Russie </s>             \n",
      "5  | has                  | T  | <ss> has </se>                 | <s> a </s>                     \n",
      "6  | tightened            | T  | <ss> tightened </se>           | <s> la pression </s>           \n",
      "7  | its                  | T  | <ss> its </se>                 | <s> son </s>                   \n",
      "8  | control              | T  | <ss> control </se>             | <s> contrle </s>              \n",
      "9  | of                   | T  | <ss> of </se>                  | <s> de </s>                    \n",
      "10 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "11 | separatist           | T  | <ss> separatist </se>          | <s> la pression </s>           \n",
      "12 | enclaves             | T  | <ss> enclaves </se>            | <s> la distribution </s>       \n",
      "13 | of                   | T  | <ss> of </se>                  | <s> de </s>                    \n",
      "14 | South                | T  | <ss> South </se>               | <s> la </s>                    \n",
      "15 | Ossetia              | T  | <ss> Ossetia </se>             | <s> la pression </s>           \n",
      "16 | and                  | T  | <ss> and </se>                 | <s> et </s>                    \n",
      "17 | Abkhazia             | T  | <ss> Abkhazia </se>            | <s> la dictature </s>          \n",
      "18 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "19 | it                   | T  | <ss> it </se>                  | <s> il </s>                    \n",
      "20 | shattered            | T  | <ss> shattered </se>           | <s> vers </s>                  \n",
      "21 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "22 | Georgian             | T  | <ss> Georgian </se>            | <s> la pression </s>           \n",
      "23 | military             | T  | <ss> military </se>            | <s> la puissance militaire </s> \n",
      "24 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "25 | grievously           | T  | <ss> grievously </se>          | <s> la pression </s>           \n",
      "26 | damaged              | T  | <ss> damaged </se>             | <s> la distribution </s>       \n",
      "27 | Georgia              | T  | <ss> Georgia </se>             | <s> l  emploi </s>            \n",
      "28 |                     | T  | <ss>  </se>                   | <s> la </s>                    \n",
      "29 | s                    | T  | <ss> s </se>                   | <s> afin </s>                  \n",
      "30 | economy              | T  | <ss> economy </se>             | <s> l  conomie </s>          \n",
      "31 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "32 | and                  | T  | <ss> and </se>                 | <s> et </s>                    \n",
      "33 | stirred              | T  | <ss> stirred </se>             | <s> vers </s>                  \n",
      "34 | up                   | T  | <ss> up </se>                  | <s> afin </s>                  \n",
      "35 | discord              | T  | <ss> discord </se>             | <s> vers </s>                  \n",
      "36 | within               | T  | <ss> within </se>              | <s> dans </s>                  \n",
      "37 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "38 | Western              | T  | <ss> Western </se>             | <s> la </s>                    \n",
      "39 | alliance             | T  | <ss> alliance </se>            | <s> alliance </s>              \n",
      "40 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | for                  | T  | <ss> for </se>                 | <s> pour </s>                  \n",
      "1  | three                | T  | <ss> three </se>               | <s> l  </s>                   \n",
      "2  | years                | T  | <ss> years </se>               | <s> annes </s>                \n",
      "3  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "4  | it                   | T  | <ss> it </se>                  | <s> il </s>                    \n",
      "5  | has                  | T  | <ss> has </se>                 | <s> a </s>                     \n",
      "6  | tried                | T  | <ss> tried </se>               | <s> vers </s>                  \n",
      "7  | every                | T  | <ss> every </se>               | <s> tous les </s>              \n",
      "8  | conceivable          | T  | <ss> conceivable </se>         | <s> la distribution </s>       \n",
      "9  | tactic               | T  | <ss> tactic </se>              | <s> la pression </s>           \n",
      "10 | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "11 | bring                | T  | <ss> bring </se>               | <s> suivre </s>                \n",
      "12 | Saakashvili          | T  | <ss> Saakashvili </se>         | <s> la pression </s>           \n",
      "13 | down                 | T  | <ss> down </se>                | <s> vers </s>                  \n",
      "14 |                     | T  | <ss>  </se>                   | <s> , </s>                     \n",
      "15 | fomenting            | T  | <ss> fomenting </se>           | <s> la pression </s>           \n",
      "16 | a                    | T  | <ss> a </se>                   | <s> un </s>                    \n",
      "17 | domestic             | T  | <ss> domestic </se>            | <s> essentiel </s>             \n",
      "18 | uprising             | T  | <ss> uprising </se>            | <s> la distribution </s>       \n",
      "19 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "20 | imposing             | T  | <ss> imposing </se>            | <s> vers </s>                  \n",
      "21 | an                   | T  | <ss> an </se>                  | <s> un </s>                    \n",
      "22 | economic             | T  | <ss> economic </se>            | <s> la conomiques </s>        \n",
      "23 | blockade             | T  | <ss> blockade </se>            | <s> la distribution </s>       \n",
      "24 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "25 | beefing              | T  | <ss> beefing </se>             | <s> la pression </s>           \n",
      "26 | up                   | T  | <ss> up </se>                  | <s> afin </s>                  \n",
      "27 | its                  | T  | <ss> its </se>                 | <s> son </s>                   \n",
      "28 | forces               | T  | <ss> forces </se>              | <s> les forces </s>            \n",
      "29 | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "30 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "31 | enclaves             | T  | <ss> enclaves </se>            | <s> la distribution </s>       \n",
      "32 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "33 | and                  | T  | <ss> and </se>                 | <s> et </s>                    \n",
      "34 | finally              | T  | <ss> finally </se>             | <s> finalement </s>            \n",
      "35 | a                    | T  | <ss> a </se>                   | <s> un </s>                    \n",
      "36 | war                  | T  | <ss> war </se>                 | <s> guerre </s>                \n",
      "37 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | yet                  | T  | <ss> yet </se>                 | <s> encore </s>                \n",
      "1  | Georgia              | T  | <ss> Georgia </se>             | <s> l  emploi </s>            \n",
      "2  |                     | T  | <ss>  </se>                   | <s> la </s>                    \n",
      "3  | s                    | T  | <ss> s </se>                   | <s> afin </s>                  \n",
      "4  | president            | T  | <ss> president </se>           | <s> prsident </s>             \n",
      "5  | remains              | T  | <ss> remains </se>             | <s> reste </s>                 \n",
      "6  | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "7  | power                | T  | <ss> power </se>               | <s> puissance </s>             \n",
      "8  | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | here                 | T  | <ss> here </se>                | <s> succs </s>                \n",
      "1  | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "2  | Tbilisi              | T  | <ss> Tbilisi </se>             | <s> la impossible </s>         \n",
      "3  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "4  | tension              | T  | <ss> tension </se>             | <s> les tensions </s>          \n",
      "5  | is                   | T  | <ss> is </se>                  | <s> est </s>                   \n",
      "6  | understandably       | T  | <ss> understandably </se>      | <s> vers </s>                  \n",
      "7  | high                 | T  | <ss> high </se>                | <s> haut </s>                  \n",
      "8  | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | Russian              | T  | <ss> Russian </se>             | <s> la Syrie </s>              \n",
      "1  | tanks                | T  | <ss> tanks </se>               | <s> la formation </s>          \n",
      "2  | are                  | T  | <ss> are </se>                 | <s> sont </s>                  \n",
      "3  | less                 | T  | <ss> less </se>                | <s> moins </s>                 \n",
      "4  | than                 | T  | <ss> than </se>                | <s> que </s>                   \n",
      "5  | 25                   | T  | <ss> 25 </se>                  | <s> choix </s>                 \n",
      "6  | miles                | T  | <ss> miles </se>               | <s> la terreur </s>            \n",
      "7  | away                 | T  | <ss> away </se>                | <s> vers </s>                  \n",
      "8  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "9  | and                  | T  | <ss> and </se>                 | <s> et </s>                    \n",
      "10 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "11 | wheat                | T  | <ss> wheat </se>               | <s> la distribution </s>       \n",
      "12 | fields               | T  | <ss> fields </se>              | <s> la peur </s>               \n",
      "13 | along                | T  | <ss> along </se>               | <s> ainsi </s>                 \n",
      "14 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "15 | main                 | T  | <ss> main </se>                | <s> principal </s>             \n",
      "16 | road                 | T  | <ss> road </se>                | <s> vers </s>                  \n",
      "17 | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "18 | Gori                 | T  | <ss> Gori </se>                | <s> la pression </s>           \n",
      "19 | were                 | T  | <ss> were </se>                | <s> taient </s>               \n",
      "20 | ablaze               | T  | <ss> ablaze </se>              | <s> vers </s>                  \n",
      "21 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "22 | set                  | T  | <ss> set </se>                 | <s> grce </s>                 \n",
      "23 | on                   | T  | <ss> on </se>                  | <s> sur </s>                   \n",
      "24 | fire                 | T  | <ss> fire </se>                | <s> le cas </s>                \n",
      "25 | by                   | T  | <ss> by </se>                  | <s> par </s>                   \n",
      "26 | Russian              | T  | <ss> Russian </se>             | <s> la Syrie </s>              \n",
      "27 | troops               | T  | <ss> troops </se>              | <s> les </s>                   \n",
      "28 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "29 | as                   | T  | <ss> as </se>                  | <s> comme </s>                 \n",
      "30 | I                    | T  | <ss> I </se>                   | <s> j  ai </s>                \n",
      "31 | drove                | T  | <ss> drove </se>               | <s> vers </s>                  \n",
      "32 | through              | T  | <ss> through </se>             | <s> par </s>                   \n",
      "33 | Russian              | T  | <ss> Russian </se>             | <s> la Syrie </s>              \n",
      "34 | checkpoints          | T  | <ss> checkpoints </se>         | <s> la distribution </s>       \n",
      "35 | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "36 | get                  | T  | <ss> get </se>                 | <s> vers </s>                  \n",
      "37 | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "38 | that                 | T  | <ss> that </se>                | <s> que </s>                   \n",
      "39 | deserted             | T  | <ss> deserted </se>            | <s> la pression </s>           \n",
      "40 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "41 | occupied             | T  | <ss> occupied </se>            | <s> vers </s>                  \n",
      "42 | city                 | T  | <ss> city </se>                | <s> prts </s>                 \n",
      "43 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | (                    | T  | <ss> ( </se>                   | <s> ( </s>                     \n",
      "1  | most                 | T  | <ss> most </se>                | <s> plus </s>                  \n",
      "2  | memorable            | T  | <ss> memorable </se>           | <s> la pression </s>           \n",
      "3  | sight                | T  | <ss> sight </se>               | <s> vers </s>                  \n",
      "4  | :                    | T  | <ss> : </se>                   | <s> : </s>                     \n",
      "5  | drunken              | T  | <ss> drunken </se>             | <s> la pression </s>           \n",
      "6  | Russian              | T  | <ss> Russian </se>             | <s> la Syrie </s>              \n",
      "7  | soldiers             | T  | <ss> soldiers </se>            | <s> soldats </s>               \n",
      "8  | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "9  | stolen               | T  | <ss> stolen </se>              | <s> la pression </s>           \n",
      "10 | Georgian             | T  | <ss> Georgian </se>            | <s> la pression </s>           \n",
      "11 | uniforms             | T  | <ss> uniforms </se>            | <s> la pression </s>           \n",
      "12 |                     | T  | <ss>  </se>                   | <s> , </s>                     \n",
      "13 |                     | T  | <ss>  </se>                   | <s>  </s>                     \n",
      "14 | because              | T  | <ss> because </se>             | <s> car </s>                   \n",
      "15 | they                 | T  | <ss> they </se>                | <s> ils </s>                   \n",
      "16 | are                  | T  | <ss> are </se>                 | <s> sont </s>                  \n",
      "17 | better               | T  | <ss> better </se>              | <s> mieux </s>                 \n",
      "18 | than                 | T  | <ss> than </se>                | <s> que </s>                   \n",
      "19 | ours                 | T  | <ss> ours </se>                | <s> la pression </s>           \n",
      "20 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "21 |                     | T  | <ss>  </se>                   | <s>  </s>                     \n",
      "22 | )                    | T  | <ss> ) </se>                   | <s> ) </s>                     \n",
      "====================================================================================\n",
      "0  | Russia               | T  | <ss> Russia </se>              | <s> la Russie </s>             \n",
      "1  |                     | T  | <ss>  </se>                   | <s> la </s>                    \n",
      "2  | s                    | T  | <ss> s </se>                   | <s> afin </s>                  \n",
      "3  | invasion             | T  | <ss> invasion </se>            | <s> devenir </s>               \n",
      "4  | of                   | T  | <ss> of </se>                  | <s> de </s>                    \n",
      "5  | Georgia              | T  | <ss> Georgia </se>             | <s> l  emploi </s>            \n",
      "6  | has                  | T  | <ss> has </se>                 | <s> a </s>                     \n",
      "7  | reshaped             | T  | <ss> reshaped </se>            | <s> la pression </s>           \n",
      "8  | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "9  | strategic            | T  | <ss> strategic </se>           | <s> la </s>                    \n",
      "10 | landscape            | T  | <ss> landscape </se>           | <s> vers </s>                  \n",
      "11 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | but                  | T  | <ss> but </se>                 | <s> mais </s>                  \n",
      "1  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "2  | as                   | T  | <ss> as </se>                  | <s> comme </s>                 \n",
      "3  | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "4  | West                 | T  | <ss> West </se>                | <s> Occident </s>              \n",
      "5  | debates              | T  | <ss> debates </se>             | <s> les bnfices </s>         \n",
      "6  | how                  | T  | <ss> how </se>                 | <s> comment </s>               \n",
      "7  | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "8  |                     | T  | <ss>  </se>                   | <s>  </s>                     \n",
      "9  | punish               | T  | <ss> punish </se>              | <s> vers </s>                  \n",
      "10 | Russia               | T  | <ss> Russia </se>              | <s> la Russie </s>             \n",
      "11 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "12 |                     | T  | <ss>  </se>                   | <s>  </s>                     \n",
      "13 | it                   | T  | <ss> it </se>                  | <s> il </s>                    \n",
      "14 | is                   | T  | <ss> is </se>                  | <s> est </s>                   \n",
      "15 | vital                | T  | <ss> vital </se>               | <s> la scurit </s>           \n",
      "16 | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "17 | remember             | T  | <ss> remember </se>            | <s> vers </s>                  \n",
      "18 | that                 | T  | <ss> that </se>                | <s> que </s>                   \n",
      "19 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "20 | main                 | T  | <ss> main </se>                | <s> principal </s>             \n",
      "21 | front                | T  | <ss> front </se>               | <s> grce </s>                 \n",
      "22 | is                   | T  | <ss> is </se>                  | <s> est </s>                   \n",
      "23 | still                | T  | <ss> still </se>               | <s> encore </s>                \n",
      "24 | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "25 | Georgia              | T  | <ss> Georgia </se>             | <s> l  emploi </s>            \n",
      "26 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | talk                 | T  | <ss> talk </se>                | <s> montre </s>                \n",
      "1  | about                | T  | <ss> about </se>               | <s> sur </s>                   \n",
      "2  | taking               | T  | <ss> taking </se>              | <s> vers </s>                  \n",
      "3  | away                 | T  | <ss> away </se>                | <s> vers </s>                  \n",
      "4  | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "5  | 2014                 | T  | <ss> 2014 </se>                | <s> vers </s>                  \n",
      "6  | Winter               | T  | <ss> Winter </se>              | <s> la pression </s>           \n",
      "7  | Olympics             | T  | <ss> Olympics </se>            | <s> la cration </s>           \n",
      "8  | or                   | T  | <ss> or </se>                  | <s> ou </s>                    \n",
      "9  | ejecting             | T  | <ss> ejecting </se>            | <s> la pression </s>           \n",
      "10 | Russia               | T  | <ss> Russia </se>              | <s> la Russie </s>             \n",
      "11 | from                 | T  | <ss> from </se>                | <s> de </s>                    \n",
      "12 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "13 | G-8                  | T  | <ss> G-8 </se>                 | <s> cas </s>                   \n",
      "14 | may                  | T  | <ss> may </se>                 | <s> pourrait </s>              \n",
      "15 | (                    | T  | <ss> ( </se>                   | <s> ( </s>                     \n",
      "16 | or                   | T  | <ss> or </se>                  | <s> ou </s>                    \n",
      "17 | may                  | T  | <ss> may </se>                 | <s> pourrait </s>              \n",
      "18 | not                  | T  | <ss> not </se>                 | <s> pas </s>                   \n",
      "19 | )                    | T  | <ss> ) </se>                   | <s> ) </s>                     \n",
      "20 | have                 | T  | <ss> have </se>                | <s> ont </s>                   \n",
      "21 | some                 | T  | <ss> some </se>                | <s> certains </s>              \n",
      "22 | effect               | T  | <ss> effect </se>              | <s> effet </s>                 \n",
      "23 | on                   | T  | <ss> on </se>                  | <s> sur </s>                   \n",
      "24 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "25 | Kremlin              | T  | <ss> Kremlin </se>             | <s> le Kremlin </s>            \n",
      "26 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "27 | but                  | T  | <ss> but </se>                 | <s> mais </s>                  \n",
      "28 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "29 | most                 | T  | <ss> most </se>                | <s> plus </s>                  \n",
      "30 | important            | T  | <ss> important </se>           | <s> plus </s>                  \n",
      "31 | thing                | T  | <ss> thing </se>               | <s> vers </s>                  \n",
      "32 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "33 | West                 | T  | <ss> West </se>                | <s> Occident </s>              \n",
      "34 | can                  | T  | <ss> can </se>                 | <s> peuvent </s>               \n",
      "35 | do                   | T  | <ss> do </se>                  | <s> faire </s>                 \n",
      "36 | now                  | T  | <ss> now </se>                 | <s> maintenant </s>            \n",
      "37 | is                   | T  | <ss> is </se>                  | <s> est </s>                   \n",
      "38 | strengthen           | T  | <ss> strengthen </se>          | <s> renforcer </s>             \n",
      "39 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "40 | government           | T  | <ss> government </se>          | <s> gouvernement </s>          \n",
      "41 | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "42 | Tbilisi              | T  | <ss> Tbilisi </se>             | <s> la impossible </s>         \n",
      "43 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "1  | equation             | T  | <ss> equation </se>            | <s> la pression </s>           \n",
      "2  | is                   | T  | <ss> is </se>                  | <s> est </s>                   \n",
      "3  | simple               | T  | <ss> simple </se>              | <s> simple </s>                \n",
      "4  | :                    | T  | <ss> : </se>                   | <s> : </s>                     \n",
      "5  | if                   | T  | <ss> if </se>                  | <s> si </s>                    \n",
      "6  | Saakashvili          | T  | <ss> Saakashvili </se>         | <s> la pression </s>           \n",
      "7  | survives             | T  | <ss> survives </se>            | <s> la distribution </s>       \n",
      "8  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "9  | Vladimir             | T  | <ss> Vladimir </se>            | <s> nos </s>                   \n",
      "10 | Putin                | T  | <ss> Putin </se>               | <s> Poutine </s>               \n",
      "11 | loses                | T  | <ss> loses </se>               | <s> vers </s>                  \n",
      "12 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "1  | intense              | T  | <ss> intense </se>             | <s> la dette </s>              \n",
      "2  | personal             | T  | <ss> personal </se>            | <s> Cuba </s>                  \n",
      "3  | hatred               | T  | <ss> hatred </se>              | <s> les dangers </s>           \n",
      "4  | between              | T  | <ss> between </se>             | <s> entre </s>                 \n",
      "5  | these                | T  | <ss> these </se>               | <s> ces </s>                   \n",
      "6  | men                  | T  | <ss> men </se>                 | <s> les hommes </s>            \n",
      "7  | overlays             | T  | <ss> overlays </se>            | <s> la pression </s>           \n",
      "8  | two                  | T  | <ss> two </se>                 | <s> deux </s>                  \n",
      "9  | centuries            | T  | <ss> centuries </se>           | <s> sicles </s>               \n",
      "10 | of                   | T  | <ss> of </se>                  | <s> de </s>                    \n",
      "11 | tortured             | T  | <ss> tortured </se>            | <s> la distribution </s>       \n",
      "12 | history              | T  | <ss> history </se>             | <s> l  histoire </s>          \n",
      "13 | between              | T  | <ss> between </se>             | <s> entre </s>                 \n",
      "14 | Russia               | T  | <ss> Russia </se>              | <s> la Russie </s>             \n",
      "15 | and                  | T  | <ss> and </se>                 | <s> et </s>                    \n",
      "16 | Georgia              | T  | <ss> Georgia </se>             | <s> l  emploi </s>            \n",
      "17 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | many                 | T  | <ss> many </se>                | <s> plusieurs </s>             \n",
      "1  | people               | T  | <ss> people </se>              | <s> les individus </s>         \n",
      "2  | report               | T  | <ss> report </se>              | <s> rapport </s>               \n",
      "3  | that                 | T  | <ss> that </se>                | <s> que </s>                   \n",
      "4  | Putin                | T  | <ss> Putin </se>               | <s> Poutine </s>               \n",
      "5  | simply               | T  | <ss> simply </se>              | <s> simplement </s>            \n",
      "6  |                     | T  | <ss>  </se>                   | <s>  </s>                     \n",
      "7  | loses                | T  | <ss> loses </se>               | <s> vers </s>                  \n",
      "8  | it                   | T  | <ss> it </se>                  | <s> il </s>                    \n",
      "9  |                     | T  | <ss>  </se>                   | <s>  </s>                     \n",
      "10 | when                 | T  | <ss> when </se>                | <s> quand </s>                 \n",
      "11 | discussing           | T  | <ss> discussing </se>          | <s> la pression </s>           \n",
      "12 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "13 | upstart              | T  | <ss> upstart </se>             | <s> la pression </s>           \n",
      "14 | Saakashvili          | T  | <ss> Saakashvili </se>         | <s> la pression </s>           \n",
      "15 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "16 | who                  | T  | <ss> who </se>                 | <s> qui </s>                   \n",
      "17 | led                  | T  | <ss> led </se>                 | <s> ont </s>                   \n",
      "18 | his                  | T  | <ss> his </se>                 | <s> son </s>                   \n",
      "19 | country              | T  | <ss> country </se>             | <s> pays </s>                  \n",
      "20 | from                 | T  | <ss> from </se>                | <s> de </s>                    \n",
      "21 | near                 | T  | <ss> near </se>                | <s> socits </s>              \n",
      "22 | bankruptcy           | T  | <ss> bankruptcy </se>          | <s> vers </s>                  \n",
      "23 | into                 | T  | <ss> into </se>                | <s> dans </s>                  \n",
      "24 | a                    | T  | <ss> a </se>                   | <s> un </s>                    \n",
      "25 | golden               | T  | <ss> golden </se>              | <s> la distribution </s>       \n",
      "26 | age                  | T  | <ss> age </se>                 | <s> viennent </s>              \n",
      "27 | of                   | T  | <ss> of </se>                  | <s> de </s>                    \n",
      "28 | economic             | T  | <ss> economic </se>            | <s> la conomiques </s>        \n",
      "29 | growth               | T  | <ss> growth </se>              | <s> croissance </s>            \n",
      "30 | and                  | T  | <ss> and </se>                 | <s> et </s>                    \n",
      "31 | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "32 | world                | T  | <ss> world </se>               | <s> monde </s>                 \n",
      "33 |                     | T  | <ss>  </se>                   | <s> la </s>                    \n",
      "34 | s                    | T  | <ss> s </se>                   | <s> afin </s>                  \n",
      "35 | highest              | T  | <ss> highest </se>             | <s> plus </s>                  \n",
      "36 | rate                 | T  | <ss> rate </se>                | <s> taux </s>                  \n",
      "37 | of                   | T  | <ss> of </se>                  | <s> de </s>                    \n",
      "38 | foreign              | T  | <ss> foreign </se>             | <s> les </s>                   \n",
      "39 | direct               | T  | <ss> direct </se>              | <s> le monde </s>              \n",
      "40 | investment           | T  | <ss> investment </se>          | <s> investissements </s>       \n",
      "41 | relative             | T  | <ss> relative </se>            | <s> ce </s>                    \n",
      "42 | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "43 | GDP                  | T  | <ss> GDP </se>                 | <s> PIB </s>                   \n",
      "44 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | all                  | T  | <ss> all </se>                 | <s> tous les </s>              \n",
      "1  | this                 | T  | <ss> this </se>                | <s> cela </s>                  \n",
      "2  | has                  | T  | <ss> has </se>                 | <s> a </s>                     \n",
      "3  | been                 | T  | <ss> been </se>                | <s> t </s>                   \n",
      "4  | halted               | T  | <ss> halted </se>              | <s> augmenter </s>             \n",
      "5  | by                   | T  | <ss> by </se>                  | <s> par </s>                   \n",
      "6  | Russian              | T  | <ss> Russian </se>             | <s> la Syrie </s>              \n",
      "7  | tanks                | T  | <ss> tanks </se>               | <s> la formation </s>          \n",
      "8  | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | the                  | T  | <ss> the </se>                 | <s> la </s>                    \n",
      "1  | Kremlin              | T  | <ss> Kremlin </se>             | <s> le Kremlin </s>            \n",
      "2  | has                  | T  | <ss> has </se>                 | <s> a </s>                     \n",
      "3  | probably             | T  | <ss> probably </se>            | <s> ceux </s>                  \n",
      "4  | lost                 | T  | <ss> lost </se>                | <s> perdu perdu perdu perdu perdu perdu perdu perdu perdu perdu perdu </s> \n",
      "5  | its                  | T  | <ss> its </se>                 | <s> son </s>                   \n",
      "6  | chance               | T  | <ss> chance </se>              | <s> chance </s>                \n",
      "7  | to                   | T  | <ss> to </se>                  | <s>  </s>                     \n",
      "8  | remove               | T  | <ss> remove </se>              | <s> la pression </s>           \n",
      "9  | Saakashvili          | T  | <ss> Saakashvili </se>         | <s> la pression </s>           \n",
      "10 | by                   | T  | <ss> by </se>                  | <s> par </s>                   \n",
      "11 | overt                | T  | <ss> overt </se>               | <s> vers </s>                  \n",
      "12 | force                | T  | <ss> force </se>               | <s> la force </s>              \n",
      "13 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "14 | although             | T  | <ss> although </se>            | <s> que </s>                   \n",
      "15 | sinister             | T  | <ss> sinister </se>            | <s> vers </s>                  \n",
      "16 | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "17 | more                 | T  | <ss> more </se>                | <s> plus </s>                  \n",
      "18 | stealthy             | T  | <ss> stealthy </se>            | <s> la pression </s>           \n",
      "19 | means                | T  | <ss> means </se>               | <s> doivent </s>               \n",
      "20 | cannot               | T  | <ss> cannot </se>              | <s> ne peut pas </s>           \n",
      "21 | be                   | T  | <ss> be </se>                  | <s> tre </s>                  \n",
      "22 | ruled                | T  | <ss> ruled </se>               | <s> Hong </s>                  \n",
      "23 | out                  | T  | <ss> out </se>                 | <s> tre </s>                  \n",
      "24 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "0  | having               | T  | <ss> having </se>              | <s> afin </s>                  \n",
      "1  | just                 | T  | <ss> just </se>                | <s> juste titre </s>           \n",
      "2  | dined                | T  | <ss> dined </se>               | <s> la pression </s>           \n",
      "3  | with                 | T  | <ss> with </se>                | <s> avec </s>                  \n",
      "4  | him                  | T  | <ss> him </se>                 | <s> la </s>                    \n",
      "5  | in                   | T  | <ss> in </se>                  | <s> dans </s>                  \n",
      "6  | a                    | T  | <ss> a </se>                   | <s> un </s>                    \n",
      "7  | public               | T  | <ss> public </se>              | <s> public </s>                \n",
      "8  | restaurant           | T  | <ss> restaurant </se>          | <s> la distribution </s>       \n",
      "9  | ,                    | T  | <ss> , </se>                   | <s> , </s>                     \n",
      "10 | I                    | T  | <ss> I </se>                   | <s> j  ai </s>                \n",
      "11 | wish                 | T  | <ss> wish </se>                | <s> grce  </s>               \n",
      "12 | his                  | T  | <ss> his </se>                 | <s> son </s>                   \n",
      "13 | security             | T  | <ss> security </se>            | <s> la scurit </s>           \n",
      "14 | was                  | T  | <ss> was </se>                 | <s> fut </s>                   \n",
      "15 | a                    | T  | <ss> a </se>                   | <s> un </s>                    \n",
      "16 | little               | T  | <ss> little </se>              | <s> pas </s>                   \n",
      "17 | tighter              | T  | <ss> tighter </se>             | <s> la pression </s>           \n",
      "18 | .                    | T  | <ss> . </se>                   | <s> . </s>                     \n",
      "====================================================================================\n",
      "Action prediction accuracy : 0.680851% \n",
      "Action prediction accuracy Translate only : 0.680851% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[u'\\xe0',\n",
       "   u'\\xeatre',\n",
       "   u'comment',\n",
       "   u',',\n",
       "   u'la',\n",
       "   u'Russie',\n",
       "   u'a',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'son',\n",
       "   u'contr\\xf4le',\n",
       "   u'de',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'de',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'et',\n",
       "   u'la',\n",
       "   u'dictature',\n",
       "   u'.',\n",
       "   u'il',\n",
       "   u'vers',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'puissance',\n",
       "   u'militaire',\n",
       "   u',',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'emploi',\n",
       "   u'la',\n",
       "   u'afin',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'\\xe9conomie',\n",
       "   u',',\n",
       "   u'et',\n",
       "   u'vers',\n",
       "   u'afin',\n",
       "   u'vers',\n",
       "   u'dans',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'alliance',\n",
       "   u'.'],\n",
       "  [u'pour',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'ann\\xe9es',\n",
       "   u',',\n",
       "   u'il',\n",
       "   u'a',\n",
       "   u'vers',\n",
       "   u'tous',\n",
       "   u'les',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'\\xe0',\n",
       "   u'suivre',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'vers',\n",
       "   u',',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'un',\n",
       "   u'essentiel',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u',',\n",
       "   u'vers',\n",
       "   u'un',\n",
       "   u'la',\n",
       "   u'\\xe9conomiques',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u',',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'afin',\n",
       "   u'son',\n",
       "   u'les',\n",
       "   u'forces',\n",
       "   u'dans',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u',',\n",
       "   u'et',\n",
       "   u'finalement',\n",
       "   u'un',\n",
       "   u'guerre',\n",
       "   u'.'],\n",
       "  [u'encore',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'emploi',\n",
       "   u'la',\n",
       "   u'afin',\n",
       "   u'pr\\xe9sident',\n",
       "   u'reste',\n",
       "   u'dans',\n",
       "   u'puissance',\n",
       "   u'.'],\n",
       "  [u'succ\\xe8s',\n",
       "   u'dans',\n",
       "   u'la',\n",
       "   u'impossible',\n",
       "   u',',\n",
       "   u'les',\n",
       "   u'tensions',\n",
       "   u'est',\n",
       "   u'vers',\n",
       "   u'haut',\n",
       "   u'.'],\n",
       "  [u'la',\n",
       "   u'Syrie',\n",
       "   u'la',\n",
       "   u'formation',\n",
       "   u'sont',\n",
       "   u'moins',\n",
       "   u'que',\n",
       "   u'choix',\n",
       "   u'la',\n",
       "   u'terreur',\n",
       "   u'vers',\n",
       "   u',',\n",
       "   u'et',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'la',\n",
       "   u'peur',\n",
       "   u'ainsi',\n",
       "   u'la',\n",
       "   u'principal',\n",
       "   u'vers',\n",
       "   u'\\xe0',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'\\xe9taient',\n",
       "   u'vers',\n",
       "   u',',\n",
       "   u'gr\\xe2ce',\n",
       "   u'sur',\n",
       "   u'le',\n",
       "   u'cas',\n",
       "   u'par',\n",
       "   u'la',\n",
       "   u'Syrie',\n",
       "   u'les',\n",
       "   u',',\n",
       "   u'comme',\n",
       "   u'j',\n",
       "   u'\\u2019',\n",
       "   u'ai',\n",
       "   u'vers',\n",
       "   u'par',\n",
       "   u'la',\n",
       "   u'Syrie',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'\\xe0',\n",
       "   u'vers',\n",
       "   u'\\xe0',\n",
       "   u'que',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u',',\n",
       "   u'vers',\n",
       "   u'pr\\xeats',\n",
       "   u'.'],\n",
       "  [u'(',\n",
       "   u'plus',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'vers',\n",
       "   u':',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'Syrie',\n",
       "   u'soldats',\n",
       "   u'dans',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u',',\n",
       "   u'\\xab',\n",
       "   u'car',\n",
       "   u'ils',\n",
       "   u'sont',\n",
       "   u'mieux',\n",
       "   u'que',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'.',\n",
       "   u'\\u201d',\n",
       "   u')'],\n",
       "  [u'la',\n",
       "   u'Russie',\n",
       "   u'la',\n",
       "   u'afin',\n",
       "   u'devenir',\n",
       "   u'de',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'emploi',\n",
       "   u'a',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'vers',\n",
       "   u'.'],\n",
       "  [u'mais',\n",
       "   u',',\n",
       "   u'comme',\n",
       "   u'la',\n",
       "   u'Occident',\n",
       "   u'les',\n",
       "   u'b\\xe9n\\xe9fices',\n",
       "   u'comment',\n",
       "   u'\\xe0',\n",
       "   u'\\xab',\n",
       "   u'vers',\n",
       "   u'la',\n",
       "   u'Russie',\n",
       "   u',',\n",
       "   u'\\u201d',\n",
       "   u'il',\n",
       "   u'est',\n",
       "   u'la',\n",
       "   u's\\xe9curit\\xe9',\n",
       "   u'\\xe0',\n",
       "   u'vers',\n",
       "   u'que',\n",
       "   u'la',\n",
       "   u'principal',\n",
       "   u'gr\\xe2ce',\n",
       "   u'est',\n",
       "   u'encore',\n",
       "   u'dans',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'emploi',\n",
       "   u'.'],\n",
       "  [u'montre',\n",
       "   u'sur',\n",
       "   u'vers',\n",
       "   u'vers',\n",
       "   u'la',\n",
       "   u'vers',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'cr\\xe9ation',\n",
       "   u'ou',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'Russie',\n",
       "   u'de',\n",
       "   u'la',\n",
       "   u'cas',\n",
       "   u'pourrait',\n",
       "   u'(',\n",
       "   u'ou',\n",
       "   u'pourrait',\n",
       "   u'pas',\n",
       "   u')',\n",
       "   u'ont',\n",
       "   u'certains',\n",
       "   u'effet',\n",
       "   u'sur',\n",
       "   u'la',\n",
       "   u'le',\n",
       "   u'Kremlin',\n",
       "   u',',\n",
       "   u'mais',\n",
       "   u'la',\n",
       "   u'plus',\n",
       "   u'plus',\n",
       "   u'vers',\n",
       "   u'la',\n",
       "   u'Occident',\n",
       "   u'peuvent',\n",
       "   u'faire',\n",
       "   u'maintenant',\n",
       "   u'est',\n",
       "   u'renforcer',\n",
       "   u'la',\n",
       "   u'gouvernement',\n",
       "   u'dans',\n",
       "   u'la',\n",
       "   u'impossible',\n",
       "   u'.'],\n",
       "  [u'la',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'est',\n",
       "   u'simple',\n",
       "   u':',\n",
       "   u'si',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u',',\n",
       "   u'nos',\n",
       "   u'Poutine',\n",
       "   u'vers',\n",
       "   u'.'],\n",
       "  [u'la',\n",
       "   u'la',\n",
       "   u'dette',\n",
       "   u'Cuba',\n",
       "   u'les',\n",
       "   u'dangers',\n",
       "   u'entre',\n",
       "   u'ces',\n",
       "   u'les',\n",
       "   u'hommes',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'deux',\n",
       "   u'si\\xe8cles',\n",
       "   u'de',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'histoire',\n",
       "   u'entre',\n",
       "   u'la',\n",
       "   u'Russie',\n",
       "   u'et',\n",
       "   u'l',\n",
       "   u'\\u2019',\n",
       "   u'emploi',\n",
       "   u'.'],\n",
       "  [u'plusieurs',\n",
       "   u'les',\n",
       "   u'individus',\n",
       "   u'rapport',\n",
       "   u'que',\n",
       "   u'Poutine',\n",
       "   u'simplement',\n",
       "   u'\\xab',\n",
       "   u'vers',\n",
       "   u'il',\n",
       "   u'\\u201d',\n",
       "   u'quand',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u',',\n",
       "   u'qui',\n",
       "   u'ont',\n",
       "   u'son',\n",
       "   u'pays',\n",
       "   u'de',\n",
       "   u'soci\\xe9t\\xe9s',\n",
       "   u'vers',\n",
       "   u'dans',\n",
       "   u'un',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u'viennent',\n",
       "   u'de',\n",
       "   u'la',\n",
       "   u'\\xe9conomiques',\n",
       "   u'croissance',\n",
       "   u'et',\n",
       "   u'la',\n",
       "   u'monde',\n",
       "   u'la',\n",
       "   u'afin',\n",
       "   u'plus',\n",
       "   u'taux',\n",
       "   u'de',\n",
       "   u'les',\n",
       "   u'le',\n",
       "   u'monde',\n",
       "   u'investissements',\n",
       "   u'ce',\n",
       "   u'\\xe0',\n",
       "   u'PIB',\n",
       "   u'.'],\n",
       "  [u'tous',\n",
       "   u'les',\n",
       "   u'cela',\n",
       "   u'a',\n",
       "   u'\\xe9t\\xe9',\n",
       "   u'augmenter',\n",
       "   u'par',\n",
       "   u'la',\n",
       "   u'Syrie',\n",
       "   u'la',\n",
       "   u'formation',\n",
       "   u'.'],\n",
       "  [u'la',\n",
       "   u'le',\n",
       "   u'Kremlin',\n",
       "   u'a',\n",
       "   u'ceux',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'perdu',\n",
       "   u'son',\n",
       "   u'chance',\n",
       "   u'\\xe0',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'par',\n",
       "   u'vers',\n",
       "   u'la',\n",
       "   u'force',\n",
       "   u',',\n",
       "   u'que',\n",
       "   u'vers',\n",
       "   u',',\n",
       "   u'plus',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'doivent',\n",
       "   u'ne',\n",
       "   u'peut',\n",
       "   u'pas',\n",
       "   u'\\xeatre',\n",
       "   u'Hong',\n",
       "   u'\\xeatre',\n",
       "   u'.'],\n",
       "  [u'afin',\n",
       "   u'juste',\n",
       "   u'titre',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'avec',\n",
       "   u'la',\n",
       "   u'dans',\n",
       "   u'un',\n",
       "   u'public',\n",
       "   u'la',\n",
       "   u'distribution',\n",
       "   u',',\n",
       "   u'j',\n",
       "   u'\\u2019',\n",
       "   u'ai',\n",
       "   u'gr\\xe2ce',\n",
       "   u'\\xe0',\n",
       "   u'son',\n",
       "   u'la',\n",
       "   u's\\xe9curit\\xe9',\n",
       "   u'fut',\n",
       "   u'un',\n",
       "   u'pas',\n",
       "   u'la',\n",
       "   u'pression',\n",
       "   u'.']],\n",
       " 0.6808510638297872,\n",
       " 0.6808510638297872)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155362"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4661"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147593"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
