{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import codecs\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr1/home/ssandeep/UltraDeep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from network import LSTM\n",
    "from layer import HiddenLayer, EmbeddingLayer\n",
    "from learning_method import LearningMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_oracle = '/usr1/home/ssandeep/IncrementalMT/oracle.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [line.strip().split(' ||| ') for line in codecs.open(path_to_oracle, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "curr_sentence_pointer = 0\n",
    "for ind, line in enumerate(lines):\n",
    "    if len(line) == 1 and line[0] == '':\n",
    "        sentences.append(lines[curr_sentence_pointer:ind])\n",
    "        curr_sentence_pointer = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soure_words = set()\n",
    "for sentence in sentences:\n",
    "    source_sentence = sentence[0][0]\n",
    "    for word in source_sentence.split():\n",
    "        soure_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_words = set()\n",
    "for sentence in sentences:\n",
    "    target_sentence = sentence[1][0]\n",
    "    for word in target_sentence.split():\n",
    "        target_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    for state in sentence[2:]:\n",
    "        assert len(state) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_word2ind = {word:ind for ind, word in enumerate(soure_words)}\n",
    "source_ind2word = {ind:word for ind, word in enumerate(soure_words)}\n",
    "target_word2ind = {word:ind for ind, word in enumerate(target_words)}\n",
    "target_ind2word = {ind:word for ind, word in enumerate(target_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_word2ind['<s>'] = len(source_word2ind)\n",
    "source_word2ind['</s>'] = len(source_word2ind) + 1\n",
    "source_word2ind['$NONE$'] = len(source_word2ind) + 2\n",
    "target_word2ind['<s>'] = len(target_word2ind)\n",
    "target_word2ind['</s>'] = len(target_word2ind) + 1\n",
    "target_word2ind['$NONE$'] = len(target_word2ind) + 2\n",
    "source_ind2word[len(source_word2ind)] = '<s>'\n",
    "source_ind2word[len(source_word2ind) + 1] = '</s>'\n",
    "source_ind2word[len(source_word2ind) + 2] = '$NONE$'\n",
    "target_ind2word[len(target_word2ind)] = '<s>' \n",
    "target_ind2word[len(target_word2ind) + 1] = '</s>' \n",
    "target_ind2word[len(target_word2ind) + 2] = '$NONE$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_input = T.ivector()\n",
    "forward_context_input = T.ivector()\n",
    "target_input = T.ivector()\n",
    "action_prediction = T.scalar()\n",
    "target_output = T.ivector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data to test dimensions\n",
    "syn_stack_input = np.random.randint(low=0, high=len(source_word2ind), size=(100,)).astype(np.int32)\n",
    "syn_forward_context_input = np.random.randint(low=0, high=len(source_word2ind), size=(100,)).astype(np.int32)\n",
    "syn_action_prediction = np.random.randint(low=0, high=2, size=(1,)).astype(np.int32)\n",
    "syn_target_input = np.random.randint(low=0, high=len(target_word2ind), size=(100,)).astype(np.int32)\n",
    "syn_target_output = np.random.randint(low=0, high=len(target_word2ind), size=(101,)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural Inventory\n",
    "source_embedding = EmbeddingLayer(input_dim=len(soure_words), output_dim=128)\n",
    "stack_source_lstm = LSTM(input_dim=source_embedding.output_dim, hidden_dim=256, name='source_stack_lstm', with_batch=False)\n",
    "forward_context_lstm = LSTM(input_dim=source_embedding.output_dim, hidden_dim=256, name='source_forward_context_lstm', with_batch=False)\n",
    "target_lstm = LSTM(input_dim=stack_source_lstm.hidden_dim, hidden_dim=256, name='target_lstm', with_batch=False)\n",
    "target_embedding = EmbeddingLayer(input_dim=len(target_words), output_dim=target_lstm.input_dim)\n",
    "action_prediction_weights = HiddenLayer(input_dim=stack_source_lstm.hidden_dim + forward_context_lstm.hidden_dim, output_dim=1)\n",
    "target_word_decoding_weights = HiddenLayer(input_dim=target_lstm.hidden_dim, output_dim=len(target_word2ind), activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.random.rand(3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis(=1) out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-1407f66c19ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr1/home/ssandeep/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(a, repeats, axis)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'repeat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axis(=1) out of bounds"
     ]
    }
   ],
   "source": [
    "np.repeat(xx, 5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# The computational graph for this method\n",
    "# =====================================================\n",
    "\n",
    "# Get the embedding matrices seq_len x embdding_dim\n",
    "stack_embedding_matrix = source_embedding.link(stack_input[::-1])\n",
    "forward_contenxt_embedding_matrix = source_embedding.link(forward_context_input)\n",
    "# target_embedding_matrix = target_embedding.fprop(target_input)\n",
    "\n",
    "# Get LSTM representations of the stack, forward context\n",
    "stack_lstm_representation = stack_source_lstm.link(stack_embedding_matrix)\n",
    "forward_context_lstm_representation = forward_context_lstm.link(forward_contenxt_embedding_matrix)\n",
    "\n",
    "# Concatenate representations and make a prediction about what action to take\n",
    "concatenated_representation = T.concatenate((stack_lstm_representation, forward_context_lstm_representation))\n",
    "prediction = action_prediction_weights.link(concatenated_representation)\n",
    "\n",
    "# Compute squared-error loss between predicted action and gold action\n",
    "action_prediction_loss = ((action_prediction - prediction) ** 2).mean()\n",
    "\n",
    "# Reshape the concatenated representation from the encoder and stack it with the target\n",
    "target_embeddings = T.vertical_stack((stack_lstm_representation.reshape((1, -1))), target_embedding.link(target_input))\n",
    "\n",
    "# Get the decoded sentence\n",
    "target_lstm.link(target_embeddings)\n",
    "\n",
    "# Decode words\n",
    "decoded_words = target_word_decoding_weights.link(target_lstm.h)\n",
    "\n",
    "# Compute seq-seq loss\n",
    "decoding_loss = T.nnet.categorical_crossentropy(decoded_words, target_output).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 128)\n",
      "(100, 128)\n",
      "(256,)\n",
      "(256,)\n",
      "(512,)\n",
      "(1,)\n",
      "(101, 256)\n",
      "(256,)\n",
      "(101, 71180)\n",
      "11.1729745865\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of Output dimensions\n",
    "print stack_embedding_matrix.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input\n",
    "    }\n",
    ").shape\n",
    "\n",
    "\n",
    "print forward_contenxt_embedding_matrix.eval(\n",
    "    {\n",
    "        forward_context_input:syn_forward_context_input\n",
    "    }\n",
    ").shape\n",
    "\n",
    "print stack_lstm_representation.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input, \n",
    "    }\n",
    ").shape\n",
    "print forward_context_lstm_representation.eval(\n",
    "    {\n",
    "        forward_context_input:syn_forward_context_input,\n",
    "    }\n",
    ").shape\n",
    "print concatenated_representation.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input,\n",
    "        forward_context_input:syn_forward_context_input,\n",
    "    }\n",
    ").shape\n",
    "print prediction.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input,\n",
    "        forward_context_input:syn_forward_context_input,\n",
    "    }\n",
    ").shape\n",
    "print target_embeddings.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input,\n",
    "        target_input:syn_target_input\n",
    "    }\n",
    ").shape\n",
    "print target_lstm_representations.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input,\n",
    "        target_input:syn_target_input\n",
    "    }\n",
    ").shape\n",
    "print decoded_words.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input,\n",
    "        target_input:syn_target_input\n",
    "    }\n",
    ").shape\n",
    "print decoding_loss.eval(\n",
    "    {\n",
    "        stack_input:syn_stack_input,\n",
    "        target_input:syn_target_input,\n",
    "        target_output:syn_target_output\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_params = source_embedding.params + stack_source_lstm.params + forward_context_lstm.params + action_prediction_weights.params\n",
    "seq_seq_params = source_embedding.params + stack_source_lstm.params + target_embedding.params + target_lstm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update parameters using ADAM\n",
    "'''\n",
    "updates_action=LearningMethod(clip=5.0).get_updates('adam', action_prediction_loss, action_params)\n",
    "\n",
    "#updates_action = Optimizer().adam(\n",
    "#                action_prediction_loss,\n",
    "#                action_params,\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update parameters using ADAM\n",
    "'''\n",
    "updates_seq_seq = LearningMethod(clip=5.0).get_updates('adam', decoding_loss, seq_seq_params)\n",
    "#updates_seq_eq = Optimizer().adam(\n",
    "#                decoding_loss,\n",
    "#                seq_seq_params,\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train_action = theano.function(\n",
    "    inputs=[stack_input, forward_context_input, action_prediction],\n",
    "    outputs=[action_prediction_loss],\n",
    "    updates=updates_action\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train_seq_seq = theano.function(\n",
    "    inputs=[stack_input, target_input, target_output],\n",
    "    outputs=[decoding_loss],\n",
    "    updates=updates_seq_seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    \n",
    "    for state in sentence[2:]:\n",
    "        word = state[0]\n",
    "        action = state[1]\n",
    "        stack_state = state[2]\n",
    "        forward_context = state[3]\n",
    "        translation = state[4]\n",
    "        stack_words = [english_word2ind[word] for word in stack_state]\n",
    "        forward_context_words = [english_word2ind[word] for word in reversed(forward_context)]\n",
    "        action = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1,1,2,2,3,4,4,4,4,4,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 7), (1, 2), (2, 2), (3, 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(x).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
